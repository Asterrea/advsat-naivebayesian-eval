{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import errno\n",
    "import json\n",
    "\n",
    "\n",
    "def extractWords(filepath):\n",
    "    file = open(filepath, 'r')\n",
    "    # .lower() returns a version with all upper case characters replaced with lower case characters.\n",
    "    text = file.read().lower()\n",
    "    file.close()\n",
    "    # replaces anything that is not a lowercase letter, a space, or an apostrophe with a space:\n",
    "    text = re.sub('[^a-z]+', \" \", text)\n",
    "    words = list(text.split())\n",
    "    words = list(set(words))\n",
    "    words =  [i for i in words if len(i) >= 3] \n",
    "#     print (words)\n",
    "    return words;\n",
    "\n",
    "def make_sure_path_exists(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "            \n",
    "def writeToFile(filename, tokenWords):\n",
    "    f = open(filename, 'w')\n",
    "    json.dump(tokenWords, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenList = []\n",
    "newList = []\n",
    "\n",
    "tokenList = extractWords(\"Emails\\\\bare\\\\part1\\\\3-1msg1.txt\")\n",
    "print (\"TokenList:\")\n",
    "print (tokenList)\n",
    "\n",
    "newList = extractWords(\"Emails\\\\bare\\\\part1\\\\3-1msg2.txt\")\n",
    "print (\"New List:\")\n",
    "print (newList)\n",
    "\n",
    "tokenList.extend(newList)\n",
    "print (\"Before Set List:\")\n",
    "print (tokenList)\n",
    "tokenList = list(set(tokenList))\n",
    "\n",
    "print(\"done\")\n",
    "print (\"Final List:\")\n",
    "print (tokenList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "tokenList = []\n",
    "rootdir = \"Emails\\\\bare\"\n",
    "# pattern = re.compile(\"\\d+-\\d+msg\\d+.txt\")\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        tempList = []\n",
    "        #print os.path.join(subdir, file)\n",
    "        filepath =  subdir + os.sep + file\n",
    "        \n",
    "        print (filepath) \n",
    "        tempList = extractWords(filepath)\n",
    "        tokenList.extend(tempList)\n",
    "            \n",
    "#         if pattern.match(file):\n",
    "#             print (filepath)           \n",
    "#             tokenWords = extractWord(filepath)\n",
    "#             tempList = extractWords(filepath)\n",
    "#             tokenList.extend(tempList)\n",
    "    \n",
    "\n",
    "print (\"done\")\n",
    "tokenList = list(set(tokenList))     \n",
    "print (tokenList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = \"Features\\\\\"\n",
    "filename = \"bare_words.txt\"\n",
    "writeToFile(filepath+filename, tokenList )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "file = open(\"Emails\\\\bare\\\\part1\\\\3-1msg1.txt\", 'r')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "text = re.sub('[^a-z]+', \" \", text)\n",
    "words = list(text.split())\n",
    "words =  [i for i in words if len(i) >= 3]\n",
    "\n",
    "print (words)\n",
    "words = list(set(words))\n",
    "print(words)\n",
    "\n",
    "\n",
    "featureVector = [][2515]\n",
    "# compare the words extracted from the entire corpus to a particular document, 1 if the term occurs in the document, and 0 otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "corpus = [\"bare\", \"lemm\",\"lemm_stop\", \"stop\"]\n",
    "counter = 0\n",
    "# pattern = re.compile(\"\\d+-\\d+msg\\d+.txt\")\n",
    "for corp in corpus: \n",
    "    rootdir = \"Emails\\\\\"\n",
    "    rootdir = rootdir + corp\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            counter = counter +1\n",
    "    print (corp)\n",
    "    print (counter)\n",
    "    counter = 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tokenList)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CorpusData: \n",
    "    corpusName = \"\"\n",
    "    totalEmails = 0\n",
    "    spamEmails = 0\n",
    "    legitEmails = 0\n",
    "\n",
    "    def __init__(self, corpusName, totalEmails, spamEmails, legitEmails):\n",
    "        self.corpusName = corpusName\n",
    "        self.totalEmails = totalEmails\n",
    "        self.spamEmails = spamEmails\n",
    "        self.legitEmails = legitEmails\n",
    "        \n",
    "bare = CorpusData(\"bare\", 100, 50, 150)\n",
    "\n",
    "print (bare.corpusName, bare.totalEmails, bare.spamEmails, bare.legitEmails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "zeroArr = np.zeros((100, 50))\n",
    "\n",
    "print (zeroArr)\n",
    "zeroArr[0][1] = -3\n",
    "\n",
    "print (\"----\")\n",
    "print (zeroArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(\"\\d+-\\d+msg\\d+.txt\")\n",
    "rootdir = \"Emails\\\\bare\"\n",
    "spamEmail = []\n",
    "legitEmail = []\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if pattern.match(file):\n",
    "            legitEmail.append(extractWords(filepath))\n",
    "        else: \n",
    "            spamEmail.append(extractWords(filepath))\n",
    "            \n",
    "\n",
    "# print (legitEmail)\n",
    "\n",
    "writeToFile(\"Features\\\\bare_legitEmails.txt\", legitEmail)\n",
    "\n",
    "# import csv\n",
    "\n",
    "# with open(\"output.csv\", \"w\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerows(legitEmail)\n",
    "\n",
    "import pandas\n",
    "\n",
    "pd = pandas.DataFrame(legitEmail)\n",
    "pd.to_csv(\"mylist.csv\",  header=False,  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "getDocs = re.compile('\\[((\"\\S+\")(,\\s)*)+\\]')\n",
    "getWord = re.compile('\"\\S+\"')\n",
    "\n",
    "uniprotFileContent ='[[\"john\", \"second\", \"reduplication\", \"logical\", \"attribute\", \"what\", \"followed\", \"irrelevant\", \"date\", \"have\", \"entity\", \"semantics\", \"relevant\", \"value\", \"well\", \"support\", \"would\", \"said\", \"were\", \"has\", \"tautologous\", \"and\", \"construction\", \"name\", \"chaim\", \"there\", \"yorku\", \"which\", \"queries\", \"the\", \"natural\", \"sense\", \"either\", \"been\", \"this\", \"logic\", \"consider\", \"for\", \"related\", \"asks\", \"here\", \"from\", \"list\", \"shmendrik\", \"sun\", \"fact\", \"form\", \"thus\", \"give\", \"tautology\", \"might\", \"level\", \"zadrozny\", \"much\", \"language\", \"false\", \"subject\", \"now\", \"who\", \"michael\", \"that\", \"about\", \"anything\", \"very\", \"supplies\", \"mcnamara\", \"wlodek\", \"est\", \"interesting\", \"indistinguishable\", \"late\", \"say\", \"those\", \"dec\", \"mmorse\", \"based\", \"discussed\"], [\"namama\", \"hi\"]]'\n",
    "# uniprotFileList = []\n",
    "# uniprotFileList = myre.split(uniprotFileContent)\n",
    "\n",
    "# print (len(getDocs.findall(uniprotFileContent)))\n",
    "\n",
    "# for mat in getDocs.findall(uniprotFileContent):\n",
    "#     print (\"\\n ----\")\n",
    "#     print (mat)\n",
    "#     for w in getWord.findall(mat):\n",
    "#         print (\"!!!!\", w)\n",
    "    \n",
    "for item in uniprotFileContent:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loads all the words extracted in the 'bare' corpus / dataset\n",
    "# file = open(\"Features\\\\bare\\\\bare_words.csv\", 'r')\n",
    "# text = file.read()\n",
    "# file.close()\n",
    "# List = open(\"Features\\\\bare\\\\bare_legitEmails.csv\").read().splitlines()\n",
    "# print (List)\n",
    "      \n",
    "import csv\n",
    "filename = 'Features\\\\bare\\\\bare_legitEmails.csv'\n",
    "with open(filename, 'r') as p:\n",
    "        #reads csv into a list of lists\n",
    "        my_list = list(list(rec) for rec in csv.reader(p, delimiter=',')) \n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "for d in my_list:\n",
    "    print (\"\\n\", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load all the legit documents for the Corpus\n",
    "\n",
    "#loads all the words extracted in the 'bare' corpus / dataset\n",
    "file = open(\"Features\\\\bare\\\\bare_legitEmails.txt\", 'r')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "#the pattern to extract all the documents from the file, a document looks something like this [\"term 1\", \"term 2\", ... \"term n\"]\n",
    "getDocs = re.compile('\\[\"\\S+\",\\s\"\\S+\"\\]')\n",
    "#the pattern to extract all the words from the document, a word looks something like this: \"term 1\"\n",
    "getWord = re.compile('\"\\S+\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn\n",
    "cv = sklearn.feature_extraction.text.CountVectorizer(vocabulary=['hot', 'cold', 'old'])\n",
    "cv.fit_transform(['pease porridge hot', 'pease porridge cold', 'pease porridge in the pot', 'nine days old']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
