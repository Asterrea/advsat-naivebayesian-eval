{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< Updated upstream
    "## A Tutorial based from the paper : \"An Evaluation of Naive Bayesian Anti-Spam Filtering\""
=======
    "# This is a tutorial to replicate the process of the paper entitled: \"An Evaluation of Naive Bayesian Anti-Spam Filtering\"\n",
    "\n",
    "The objective of the paper is to filter spam emails using a Naive Bayes Techinique"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're gonna do here is the following: \n",
    "\n",
    "1. Construct the Feature Vector\n",
    "2. \n",
    "\n",
    "We will start with generating the dataset that will be used. \n",
    "We need to construct the features that will be used, now this will be evaluated based on the words found in the corpus\n",
    "The following code below, extracts the tokens / words, in the file and stores it in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import errno\n",
    "import json\n",
    "\n",
    "\n",
    "def extractWords(filepath):\n",
    "    file = open(filepath, 'r')\n",
    "    # .lower() returns a version with all upper case characters replaced with lower case characters.\n",
    "    text = file.read().lower()\n",
    "    file.close()\n",
    "    # replaces anything that is not a lowercase letter, a space, or an apostrophe with a space:\n",
    "    text = re.sub('[^a-z]+', \" \", text)\n",
    "    words = list(text.split())\n",
    "#     print (words)\n",
    "    return words;\n",
    "\n",
    "\n",
    "def make_sure_path_exists(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "            \n",
    "def writeToFile(subdir, filename, tokenWords):\n",
    "    make_sure_path_exists(subdir)\n",
    "    f = open(filename, 'w')\n",
    "    json.dump(tokenWords, f)\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "rootdir = \"Emails\\\\\"\n",
    "pattern = re.compile(\"\\d+-\\d+msg\\d+.txt\")\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #print os.path.join(subdir, file)\n",
    "        filepath =  subdir + os.sep + file\n",
    "        \n",
    "        if pattern.match(file):\n",
    "            print (filepath)\n",
    "            filename = \"Output\\\\legit\\\\\"+filepath\n",
    "            \n",
    "            tokenWords = extractWord(filepath)\n",
    "#             writeToFile(subdir, filename, tokenWords)\n",
    "            \n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The dataset provided to us has 4 directories [bare, lemm, lemm_stop, stop] with subdirectories of 10 parts. Where 9 were used as training set and 1 reserved for testing for every repetition. To reduce random variation, a ten cross-validation was done yielding the ten subdirectories.\n",
    "\n",
    "We first split this data into 90% training set for Naive Bayes prediction and 10% data for accuracy testing. Let's call these variables: train_set and test_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spilt_dataset(dataset, n_split):\n",
    "    train_set = []\n",
    "    data_copy = list(dataset) \n",
    "    while len(train_set) < int(len(dataset)*n_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To Regina : Insert reading, tokenizing, and MI computing here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
