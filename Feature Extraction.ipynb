{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The features that will be used are the words that can be found in the each document in the entire corpus. In order to extract all the features from the corpus, we first need to extract all the words in the corpus, the words will serve as a feature\n",
    "\n",
    "The following are the libraries that will be used in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imports here\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pandas\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import csv \n",
    "\n",
    "from numpy import genfromtxt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class that will hold the Data about each Corpus. \n",
    "In the paper, they used 4 different corpus\n",
    "\n",
    "1. Bare : Original Email Data\n",
    "2. Lemm : Words in the emails are lemmatized\n",
    "3. Stop : Stops words are removed from the emails\n",
    "4. Lemm_stop : Words are lemmatized and stop words are removed from the emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a class for the the Corpus data, it will store the total number of emails in the corpus, \n",
    "# along with the total number of spam and legit emails\n",
    "class CorpusData: \n",
    "    corpusName = \"\"\n",
    "    totalEmailCtr = 0\n",
    "    spamEmailCtr = 0\n",
    "    legitEmailCtr = 0\n",
    "    legitEmails = []\n",
    "    spamEmails = []\n",
    "\n",
    "    def __init__(self, corpusName, totalEmailCtr, spamEmailCtr, legitEmailCtr, legitEmailsArr, spamEmailsArr):\n",
    "        self.corpusName = corpusName\n",
    "        self.totalEmailCtr = totalEmailCtr\n",
    "        self.spamEmailCtr = spamEmailCtr\n",
    "        self.legitEmailCtr = legitEmailCtr\n",
    "        self.legitEmails = legitEmailsArr\n",
    "        self.spamEmails = spamEmailsArr\n",
    "\n",
    "# corpus list \n",
    "corpus = [\"bare\", \"lemm\",\"lemm_stop\", \"stop\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `extractWords`, extracts the words in a textfle given its filepath as its parameter\n",
    "\n",
    "`words =  [i for i in words if len(i) >= 4]` filters out the words that are less than 4 letters\n",
    "\n",
    "`words = list(set(words))` Creates a set from the list of words extracted, and duplicate words are removed\n",
    "\n",
    "\n",
    "The function `writeToFile`, write to text file given the filepath, and the data to write as its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extractWords(filepath):\n",
    "    file = open(filepath, 'r')\n",
    "    # .lower() returns a version with all upper case characters replaced with lower case characters.\n",
    "    text = file.read().lower()\n",
    "    file.close()\n",
    "    # replaces anything that is not a lowercase letter, a space, or an apostrophe with a space:\n",
    "    text = re.sub('[^a-z]+', \" \", text)\n",
    "    words = list(text.split())\n",
    "    \n",
    "     # remove duplicate words in the list\n",
    "    words = list(set(words))\n",
    "    # removes words that are less than 4 letters/characters\n",
    "    words =  [i for i in words if len(i) >= 4] \n",
    "    return words;\n",
    "\n",
    "# this functions write the list of tokens in the tokenWords parameter on the filepath passed on filename parameter \n",
    "def writeToFile(filename, tokenWords):\n",
    "    f = open(filename, 'w')\n",
    "    json.dump(tokenWords, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below traverses all of the documents that is under a given directory, we have 4 corpus, so we need to do this 4 times, one for each corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a list that will hold CorpusData Object\n",
    "corpusDataList = []\n",
    "\n",
    "# this will hold all the words in the corpus\n",
    "wordList = []\n",
    "#corpus list\n",
    "corpus = [\"bare\", \"lemm\", \"lemm_stop\", \"stop\"]\n",
    "\n",
    "#this regular expression will match all text files with this pattern (Files that has this pattern are the Legitimate Emails)\n",
    "pattern = re.compile(\"\\d+-\\d+msg\\d+.txt\")\n",
    "\n",
    "#for each corpus\n",
    "for corp in corpus:\n",
    "    legitEmailCTR = 0\n",
    "    spamCTR = 0\n",
    "    counter = 0  \n",
    "    spamEmails = []\n",
    "    legitEmails = []\n",
    "    rootdir = \"Emails/\"\n",
    "    rootdir = rootdir + corp\n",
    "    \n",
    "    #for each subdirectory in a corpus (folders - part 1 - 10)\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        #for each file in a folder\n",
    "        for file in files:   \n",
    "            tempList = []\n",
    "            filepath =  subdir + os.sep + file\n",
    "            #words are extracted from a file\n",
    "            tempList = extractWords(filepath)\n",
    "            #extracted words are added to the word list\n",
    "            wordList.extend(tempList)\n",
    "            \n",
    "            #increment total email counter for this corpus\n",
    "            counter = counter +1\n",
    "            # create a string of all the words in a document (instead of a list)\n",
    "            joinedStr = ' '.join(tempList)\n",
    "            # if file is a legitimate email\n",
    "            if pattern.match(file):    \n",
    "                #add string/file to legitimate emails list\n",
    "                legitEmails.append(joinedStr)\n",
    "                #increment legitimate email counter\n",
    "                legitEmailCTR =  legitEmailCTR + 1\n",
    "            else: \n",
    "                #add string/file to spam emails list\n",
    "                spamEmails.append(joinedStr)\n",
    "                #increment spam email counter\n",
    "                spamCTR = spamCTR + 1\n",
    "    #Update word list to remove all duplicates words in the corpus (cause, a word might occur in multiple documents/emails)     \n",
    "    wordList = list(set(wordList))     \n",
    "    \n",
    "#     #tokens/words extracted from the emails will be stored in a text file so it can be used for further analyzation\n",
    "    pd1 = pandas.DataFrame(wordList)\n",
    "    pd1.to_csv(\"Features/\"+corp+\"/\"+corp+\"_words.csv\",  header=False,  index=False)\n",
    "    pd2 = pandas.DataFrame(legitEmails)\n",
    "    pd2.to_csv(\"Features/\"+corp+\"/\"+corp+\"_legitEmails.csv\",  header=False,  index=False)\n",
    "    pd3 = pandas.DataFrame(spamEmails)\n",
    "    pd3.to_csv(\"Features/\"+corp+\"/\"+corp+\"_spamEmails.csv\",  header=False,  index=False)\n",
    "\n",
    "# create a class that will hold all relevant information about the Corpus    \n",
    "    x = CorpusData(corp, counter, spamCTR, legitEmailCTR, legitEmails, spamEmails)\n",
    "    corpusDataList.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 2D matrix of term occurrence, x = terms, y = documents, 1 = term occurred in document then 0 otherwise, \n",
    "\n",
    "We will now populate the term document matrix, we can have two approaches in doing this. \n",
    "\n",
    "1. Build our own term document matrix populator by traversing each document in a corpus, and compare each word in a document to the term list, to get its index and mark it as 1 (term occurred in document)\n",
    "\n",
    "The first approach will have the following steps:\n",
    "\n",
    "1. iterate over all the documents/emails in the corpus\n",
    "2. Each document/email extract the words\n",
    "3. Compare each word in the document to the term list\n",
    "4. tdm[term][document] = 1\n",
    "\n",
    "**MAJOR DOWNSIDE**: this will take so much time to finish executing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "done\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9e0360aad156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tdm' is not defined"
     ]
    }
   ],
   "source": [
    "#create document list\n",
    "documentList = corpusDataList[0].legitEmails\n",
    "\n",
    "#initialize document counter to 0 (first document)\n",
    "docCtr = 0\n",
    "print (\"Starting\")\n",
    "#for each document\n",
    "for doc in documentList:   \n",
    "#     print (\"---DOC---\",doc)\n",
    "    #for each word in a document\n",
    "    for word in doc:\n",
    "        # if word is in the wordlist\n",
    "#         print (\"---WORD---\",word)\n",
    "        if  word in wordList: \n",
    "            #get index of word in the word list\n",
    "            b=wordList.index(word)\n",
    "            print (\"index:\", b)\n",
    "            #update tdm to 1 (1: word occured in document)\n",
    "            tdm[b][docCtr] = 1\n",
    "    #update document counter + 1, for the next document\n",
    "    docCtr = docCtr + 1\n",
    "\n",
    "print (\"done\")\n",
    "print (tdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: or we can use a sklearn function called count vectorizer, example of what `CountVectorizer` will do\n",
    "\n",
    "given a list of Vocabulary (in our case, Term/Word List) it will create a term document matrix (array) with all of the terms mapped on the documents \n",
    "\n",
    "A sample of this library is shown in the next cell. \n",
    "1. *vocab*:  is the list of all the terms that will be used to tag the documents\n",
    "2. *doc*: The list of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ['hot', 'cold', 'old']\n",
    "#load all the term list here\n",
    "cv = sklearn.feature_extraction.text.CountVectorizer(vocabulary=vocab)\n",
    "\n",
    "doc = ['pease porridge hot', 'pease porridge cold', 'pease porridge in the pot', 'nine days old']\n",
    "#load all the documents here\n",
    "cv.fit_transform(doc).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Applying that to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are:  47661  Words\n",
      "The File that we are using for the documents is: Features/bare/bare_spamEmails.csv\n",
      "There are:  304 Spam Documents\n",
      "The File that we are using for the documents is: Features/bare/bare_legitEmails.csv\n",
      "There are:  2211 Legitimate Documents\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#load the vocabulary /word list/ term list from file\n",
    "wordList = open(\"Features/bare/bare_words.csv\").read().splitlines()\n",
    "print (\"There are: \", len(wordList), \" Words\")\n",
    "\n",
    "#load the documents here\n",
    "rootDIR = \"Features/\"\n",
    "corpus = \"bare/bare\"\n",
    "spamMail = \"_spamEmails.csv\"\n",
    "legitMail = \"_legitEmails.csv\"\n",
    "\n",
    "#create a document list\n",
    "spamDocumentList = []\n",
    "legitDocumentList = []\n",
    "\n",
    "file_name_spam = rootDIR+corpus+spamMail\n",
    "file_name_legit = rootDIR+corpus+legitMail\n",
    "\n",
    "with open(file_name_spam, 'r') as f:  \n",
    "    reader = csv.reader(f)   \n",
    "    #for each row in the file, append it the document list\n",
    "    for row in reader:\n",
    "        for doc in row:\n",
    "            spamDocumentList.append(doc)\n",
    "\n",
    "print (\"The File that we are using for the documents is:\", file_name_spam)\n",
    "print (\"There are: \", len(spamDocumentList), \"Spam Documents\")\n",
    "\n",
    "with open(file_name_legit, 'r') as f:  \n",
    "    reader = csv.reader(f)   \n",
    "    #for each row in the file, append it the document list\n",
    "    for row in reader:\n",
    "        for doc in row:\n",
    "            legitDocumentList.append(doc)\n",
    "\n",
    "print (\"The File that we are using for the documents is:\", file_name_legit)\n",
    "print (\"There are: \", len(legitDocumentList), \"Legitimate Documents\")\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have, the terms and the documents loaded from the file. We can now start creating the term document matrix, and the term document matrix to file\n",
    "\n",
    "\n",
    "term document matrix (tdm)\n",
    "1. rows : documents\n",
    "2. columns : terms/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are:  14488944  cells in the term document matrix\n",
      "There are:  105378471  cells in the term document matrix\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#use term list here\n",
    "cv = sklearn.feature_extraction.text.CountVectorizer(vocabulary=wordList)\n",
    "\n",
    "# -------------------- SPAM -----------------------\n",
    "#use documents list here\n",
    "tdmSpam = cv.fit_transform(spamDocumentList).toarray()\n",
    "print (\"There are: \", tdmSpam.size, \" cells in the term document matrix\")\n",
    "\n",
    "#create a numpy array\n",
    "aSpam = np.asarray(tdmSpam)\n",
    "#save the Term Document Matrix to CSV (so we can access it later)\n",
    "np.savetxt(\"Features/bare/spamTDM.csv\", aSpam, delimiter=\",\")\n",
    "\n",
    "\n",
    "# -------------------- LEGITIMATE -----------------------\n",
    "#use documents list here\n",
    "tdmLegit = cv.fit_transform(legitDocumentList).toarray()\n",
    "print (\"There are: \", tdmLegit.size, \" cells in the term document matrix\")\n",
    "\n",
    "#create a numpy array\n",
    "aLegit= np.asarray(tdmLegit)\n",
    "#save the Term Document Matrix to CSV (so we can access it later)\n",
    "np.savetxt(\"Features/bare/legitTDM.csv\", aLegit, delimiter=\",\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the term document matrix, we can now start computing for the Mutual Information of each term in the corpus\n",
    "\n",
    "\\begin{align}\n",
    "MI(X;C) =\\sum_{x \\epsilon {0,1}, c \\epsilon {spam,legitimate}}^{ } P(X=x, C=c) \\cdot log \\frac{P(X=x, C=c)}{P(X=x)\\cdot P(C=c)}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "*P(X=x, C=c)* : Total number of documents the word `x` occurred in documents that are class `c`\n",
    "\n",
    "*P(X=x)* : Total number of documents the word `x` occurred in the corpus\n",
    "\n",
    "*P(C=c)* : Total number of documents the are class  `c` in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam TDM :  (304, 47661)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#load the spamTDM from file\n",
    "spamTDM = genfromtxt('Features/bare/spamTDM.csv', delimiter=',')\n",
    "#prints the size of the matrix, (number of documents, number of terms)\n",
    "print(\"Spam TDM : \", spamTDM.shape)\n",
    "\n",
    "spamTermOccurCount = (spamTDM != 0).sum(0)\n",
    "print (\"done\")\n",
    "\n",
    "#create a numpy array\n",
    "spamTermCount = np.asarray(spamTermOccurCount)\n",
    "#save the Term Document Matrix to CSV (so we can access it later)\n",
    "np.savetxt(\"Features/bare/spamTermCount.csv\", spamTermCount, delimiter=\",\")\n",
    "print(\"File saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legit TDM :  (2211, 47661)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#load the legitTDM from file\n",
    "legitTDM = genfromtxt('Features/bare/legitTDM.csv', delimiter=',')\n",
    "#prints the size of the matrix, (number of documents, number of terms)\n",
    "print(\"Legit TDM : \", legitTDM.shape)\n",
    "legitTermOccurCount = (legitTDM != 0).sum(0)\n",
    "print (\"done\")\n",
    "\n",
    "\n",
    "#create a numpy array\n",
    "legitTermCount = np.asarray(legitTermOccurCount)\n",
    "#save the Term Document Matrix to CSV (so we can access it later)\n",
    "np.savetxt(\"Features/bare/legitTermCount.csv\", legitTermCount, delimiter=\",\")\n",
    "print(\"File Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all that pre-processing, now we can *finally* compute the mutual information score of the term on the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2211 304\n",
      "File Saved\n",
      "[[  2.           4.         -20.00270968]\n",
      " [  0.           2.          -6.68917749]\n",
      " [  0.           1.          -3.34458874]\n",
      " ..., \n",
      " [  0.           5.         -16.72294371]\n",
      " [  0.           2.          -6.68917749]\n",
      " [  0.           3.         -10.03376623]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "totalLegit = corpusDataList[0].legitEmailCtr\n",
    "totalSpam = corpusDataList[0].spamEmailCtr\n",
    "print (totalLegit, totalSpam)\n",
    "\n",
    "def computeMI(termSpamCount, termLegitCount):\n",
    "    totalTermCount = termSpamCount + termLegitCount\n",
    "    try:\n",
    "        classSpam = termSpamCount * math.log10(termSpamCount/(totalTermCount*totalSpam))\n",
    "    except ValueError:\n",
    "        classSpam = 0\n",
    "    \n",
    "    try:\n",
    "        classLegit = termLegitCount * math.log10(termLegitCount/(totalTermCount*totalLegit)) \n",
    "    except ValueError:\n",
    "        classLegit = 0\n",
    "        \n",
    "    return (classSpam + classLegit) \n",
    "    \n",
    "\n",
    "\n",
    "legitTermCountArr = genfromtxt('Features/bare/legitTermCount.csv', delimiter=',')\n",
    "spamTermCountArr = genfromtxt('Features/bare/spamTermCount.csv', delimiter=',')\n",
    "mi = []\n",
    "\n",
    "vfunc = np.vectorize(computeMI)\n",
    "mi = vfunc(spamTermCountArr, legitTermCountArr)\n",
    "\n",
    "final = np.vstack((spamTermCountArr,legitTermCountArr, mi)).T\n",
    "\n",
    "#save the Term Document Matrix to CSV (so we can access it later)\n",
    "np.savetxt(\"Features/bare/termMI.csv\", final, delimiter=\",\")\n",
    "print(\"File Saved\")\n",
    "\n",
    "print (final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
