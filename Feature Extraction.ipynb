{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The features that will be used are the words that can be found in the each document in the entire corpus. In order to extract all the features from the corpus, we first need to extract all the words in the corpus, the words will serve as a feature\n",
    "\n",
    "The following are the libraries that will be used in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imports here\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pandas\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import csv \n",
    "\n",
    "from numpy import genfromtxt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class that will hold the Data about each Corpus. \n",
    "In the paper, they used 4 different corpus\n",
    "\n",
    "1. Bare : Original Email Data\n",
    "2. Lemm : Words in the emails are lemmatized\n",
    "3. Stop : Stops words are removed from the emails\n",
    "4. Lemm_stop : Words are lemmatized and stop words are removed from the emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a class for the the Corpus data, it will store the total number of emails in the corpus, \n",
    "# along with the total number of spam and legit emails\n",
    "class CorpusData: \n",
    "    corpusName = \"\"\n",
    "    totalEmailCtr = 0\n",
    "    spamEmailCtr = 0\n",
    "    legitEmailCtr = 0\n",
    "\n",
    "    def __init__(self, corpusName, totalEmailCtr, spamEmailCtr, legitEmailCtr):\n",
    "        self.corpusName = corpusName\n",
    "        self.totalEmailCtr = totalEmailCtr\n",
    "        self.spamEmailCtr = spamEmailCtr\n",
    "        self.legitEmailCtr = legitEmailCtr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `extractWords`, extracts the words in a textfle given its filepath as its parameter\n",
    "\n",
    "`words =  [i for i in words if len(i) >= 4]` filters out the words that are less than 4 letters\n",
    "\n",
    "`words = list(set(words))` Creates a set from the list of words extracted, and duplicate words are removed\n",
    "\n",
    "\n",
    "The function `writeToFile`, write to text file given the filepath, and the data to write as its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extractWords(filepath):\n",
    "    file = open(filepath, 'r')\n",
    "    # .lower() returns a version with all upper case characters replaced with lower case characters.\n",
    "    text = file.read().lower()\n",
    "    file.close()\n",
    "    # replaces anything that is not a lowercase letter, a space, or an apostrophe with a space:\n",
    "    text = re.sub('[^a-z]+', \" \", text)\n",
    "    words = list(text.split())\n",
    "    \n",
    "     # remove duplicate words in the list\n",
    "    words = list(set(words))\n",
    "    # removes words that are less than 4 letters/characters\n",
    "    words =  [i for i in words if len(i) >= 4] \n",
    "    return words;\n",
    "\n",
    "# this functions write the list of tokens in the tokenWords parameter on the filepath passed on filename parameter \n",
    "def writeToFile(filename, tokenWords):\n",
    "    f = open(filename, 'w')\n",
    "    json.dump(tokenWords, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below traverses all of the documents that is under a given directory, we have 4 corpus, so we need to do this 4 times, one for each corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessCorpus(corp):    \n",
    "    # this will hold all the words in the corpus\n",
    "    wordList = []\n",
    "    #this regular expression will match all text files with this pattern (Files that has this pattern are the Legitimate Emails)\n",
    "    pattern = re.compile(\"\\d+-\\d+msg\\d+.txt\")\n",
    "\n",
    "    legitEmailCTR = 0\n",
    "    spamCTR = 0\n",
    "    counter = 0  \n",
    "    spamEmails = []\n",
    "    legitEmails = []\n",
    "    rootdir = \"Emails/\"\n",
    "    rootdir = rootdir + corp\n",
    "\n",
    "    #for each subdirectory in a corpus (folders - part 1 - 10)\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        #for each file in a folder\n",
    "        for file in files:   \n",
    "            tempList = []\n",
    "            filepath =  subdir + os.sep + file\n",
    "            #words are extracted from a file\n",
    "            tempList = extractWords(filepath)\n",
    "            #extracted words are added to the word list\n",
    "            wordList.extend(tempList)\n",
    "\n",
    "            #increment total email counter for this corpus\n",
    "            counter = counter +1\n",
    "            # create a string of all the words in a document (instead of a list)\n",
    "            joinedStr = ' '.join(tempList)\n",
    "            # if file is a legitimate email\n",
    "            if pattern.match(file):    \n",
    "                #add string/file to legitimate emails list\n",
    "                legitEmails.append(joinedStr)\n",
    "                #increment legitimate email counter\n",
    "                legitEmailCTR =  legitEmailCTR + 1\n",
    "            else: \n",
    "                #add string/file to spam emails list\n",
    "                spamEmails.append(joinedStr)\n",
    "                #increment spam email counter\n",
    "                spamCTR = spamCTR + 1\n",
    "    #Update word list to remove all duplicates words in the corpus (cause, a word might occur in multiple documents/emails)     \n",
    "    wordList = list(set(wordList))     \n",
    "\n",
    "    #tokens/words extracted from the emails will be stored in a text file so it can be used for further analyzation\n",
    "    pd1 = pandas.DataFrame(wordList)\n",
    "    pd1.to_csv(\"Features/\"+corp+\"/\"+corp+\"_words.csv\",  header=False,  index=False)\n",
    "    pd2 = pandas.DataFrame(legitEmails)\n",
    "    pd2.to_csv(\"Features/\"+corp+\"/\"+corp+\"_legitEmails.csv\",  header=False,  index=False)\n",
    "    pd3 = pandas.DataFrame(spamEmails)\n",
    "    pd3.to_csv(\"Features/\"+corp+\"/\"+corp+\"_spamEmails.csv\",  header=False,  index=False)\n",
    "\n",
    "    #create a class that will hold all relevant information about the Corpus    \n",
    "    x = CorpusData(corp, counter, spamCTR, legitEmailCTR)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 2D matrix of term occurrence, x = terms, y = documents, 1 = term occurred in document then 0 otherwise, \n",
    "\n",
    "We will now populate the term document matrix, we can have two approaches in doing this. \n",
    "\n",
    "1. Build our own term document matrix populator by traversing each document in a corpus, and compare each word in a document to the term list, to get its index and mark it as 1 (term occurred in document)\n",
    "\n",
    "The first approach will have the following steps:\n",
    "\n",
    "1. iterate over all the documents/emails in the corpus\n",
    "2. Each document/email extract the words\n",
    "3. Compare each word in the document to the term list\n",
    "4. tdm[term][document] = 1\n",
    "\n",
    "**MAJOR DOWNSIDE**: this will take so much time to finish executing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def approach1():\n",
    "    #create document list\n",
    "    documentList = corpusDataList[0].legitEmails\n",
    "\n",
    "    #initialize document counter to 0 (first document)\n",
    "    docCtr = 0\n",
    "    print (\"Starting\")\n",
    "    #for each document\n",
    "    for doc in documentList:   \n",
    "    #     print (\"---DOC---\",doc)\n",
    "        #for each word in a document\n",
    "        for word in doc:\n",
    "            # if word is in the wordlist\n",
    "    #         print (\"---WORD---\",word)\n",
    "            if  word in wordList: \n",
    "                #get index of word in the word list\n",
    "                b=wordList.index(word)\n",
    "                print (\"index:\", b)\n",
    "                #update tdm to 1 (1: word occured in document)\n",
    "                tdm[b][docCtr] = 1\n",
    "        #update document counter + 1, for the next document\n",
    "        docCtr = docCtr + 1\n",
    "\n",
    "    print (\"done\")\n",
    "    print (tdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: or we can use a sklearn function called count vectorizer, example of what `CountVectorizer` will do\n",
    "\n",
    "given a list of Vocabulary (in our case, Term/Word List) it will create a term document matrix (array) with all of the terms mapped on the documents \n",
    "\n",
    "A sample of this library is shown in the next cell. \n",
    "1. *vocab*:  is the list of all the terms that will be used to tag the documents\n",
    "2. *doc*: The list of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ['hot', 'cold', 'old']\n",
    "#load all the term list here\n",
    "cv = sklearn.feature_extraction.text.CountVectorizer(vocabulary=vocab)\n",
    "\n",
    "doc = ['pease porridge hot', 'pease porridge cold', 'pease porridge in the pot', 'nine days old']\n",
    "#load all the documents here\n",
    "cv.fit_transform(doc).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Applying that to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def approach2(corp):    \n",
    "    #load the documents here\n",
    "    rootDIR = \"Features/\"\n",
    "    corpus = corp+\"/\"+corp\n",
    "    spamMail = \"_spamEmails.csv\"\n",
    "    legitMail = \"_legitEmails.csv\"\n",
    "    \n",
    "\n",
    "    #create a document list\n",
    "    spamDocumentList = []\n",
    "    legitDocumentList = []\n",
    "\n",
    "    file_name_spam = rootDIR+corpus+spamMail\n",
    "    file_name_legit = rootDIR+corpus+legitMail\n",
    "\n",
    "    with open(file_name_spam, 'r') as f:  \n",
    "        reader = csv.reader(f)   \n",
    "        #for each row in the file, append it the document list\n",
    "        for row in reader:\n",
    "            for doc in row:\n",
    "                spamDocumentList.append(doc)\n",
    "\n",
    "    print (\"The File that we are using for the documents is:\", file_name_spam)\n",
    "    print (\"There are: \", len(spamDocumentList), \"Spam Documents\")\n",
    "\n",
    "    with open(file_name_legit, 'r') as f:  \n",
    "        reader = csv.reader(f)   \n",
    "        #for each row in the file, append it the document list\n",
    "        for row in reader:\n",
    "            for doc in row:\n",
    "                legitDocumentList.append(doc)\n",
    "\n",
    "    print (\"The File that we are using for the documents is:\", file_name_legit)\n",
    "    print (\"There are: \", len(legitDocumentList), \"Legitimate Documents\")\n",
    "    print (\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have, the terms and the documents loaded from the file. We can now start creating the term document matrix, and the term document matrix to file\n",
    "\n",
    "\n",
    "term document matrix (tdm)\n",
    "1. rows : documents\n",
    "2. columns : terms/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createTDM(corp):   \n",
    "    \n",
    "    #load the vocabulary /word list/ term list from file\n",
    "    wordList = open(\"Features/\"+corp+\"/\"+corp+\"_words.csv\").read().splitlines()\n",
    "    print (\"There are: \", len(wordList), \" Words\")\n",
    "    \n",
    "    #use term list here\n",
    "    cv = sklearn.feature_extraction.text.CountVectorizer(vocabulary=wordList)\n",
    "    \n",
    "    spamDocumentList = open(\"Features/\"+corp+\"/\"+corp+\"_spamEmails.csv\").read().splitlines()\n",
    "    legitDocumentList = open(\"Features/\"+corp+\"/\"+corp+\"_legitEmails.csv\").read().splitlines()\n",
    "    \n",
    "    # -------------------- SPAM -----------------------\n",
    "    #use documents list here\n",
    "    tdmSpam = cv.fit_transform(spamDocumentList).toarray()\n",
    "    print (\"There are: \", tdmSpam.size, \" cells in the term document matrix\")\n",
    "\n",
    "    #create a numpy array\n",
    "    aSpam = np.asarray(tdmSpam)\n",
    "    #save the Term Document Matrix to CSV (so we can access it later)\n",
    "    np.savetxt(\"Features/\"+corp+\"/spamTDM.csv\", aSpam, delimiter=\",\")\n",
    "\n",
    "\n",
    "    # -------------------- LEGITIMATE -----------------------\n",
    "    #use documents list here\n",
    "    tdmLegit = cv.fit_transform(legitDocumentList).toarray()\n",
    "    print (\"There are: \", tdmLegit.size, \" cells in the term document matrix\")\n",
    "\n",
    "    #create a numpy array\n",
    "    aLegit= np.asarray(tdmLegit)\n",
    "    #save the Term Document Matrix to CSV (so we can access it later)\n",
    "    np.savetxt(\"Features/\"+corp+\"/legitTDM.csv\", aLegit, delimiter=\",\")\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the term document matrix, we can now start computing for the Mutual Information of each term in the corpus\n",
    "\n",
    "\\begin{align}\n",
    "MI(X;C) =\\sum_{x \\epsilon {0,1}, c \\epsilon {spam,legitimate}}^{ } P(X=x, C=c) \\cdot log \\frac{P(X=x, C=c)}{P(X=x)\\cdot P(C=c)}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "*P(X=x, C=c)* : Total number of documents the word `x` occurred in documents that are class `c`\n",
    "\n",
    "*P(X=x)* : Total number of documents the word `x` occurred in the corpus\n",
    "\n",
    "*P(C=c)* : Total number of documents the are class  `c` in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processSpamTDM(corp): \n",
    "    #load the spamTDM from file\n",
    "    spamTDM = genfromtxt(\"Features/\"+corp+\"/spamTDM.csv\", delimiter=',')\n",
    "    #prints the size of the matrix, (number of documents, number of terms)\n",
    "    print(\"Spam TDM : \", spamTDM.shape)\n",
    "\n",
    "    spamTermOccurCount = (spamTDM != 0).sum(0)\n",
    "    print (\"done\")\n",
    "\n",
    "    #create a numpy array\n",
    "    spamTermCount = np.asarray(spamTermOccurCount)\n",
    "    #save the Term Document Matrix to CSV (so we can access it later)\n",
    "    np.savetxt(\"Features/\"+corp+\"/spamTermCount.csv\", spamTermCount, delimiter=\",\")\n",
    "    print(\"File saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processLegitTDM(corp):\n",
    "    #load the legitTDM from file\n",
    "    legitTDM = genfromtxt(\"Features/\"+corp+\"/legitTDM.csv\", delimiter=',')\n",
    "    #prints the size of the matrix, (number of documents, number of terms)\n",
    "    print(\"Legit TDM : \", legitTDM.shape)\n",
    "    legitTermOccurCount = (legitTDM != 0).sum(0)\n",
    "    print (\"done\")\n",
    "\n",
    "\n",
    "    #create a numpy array\n",
    "    legitTermCount = np.asarray(legitTermOccurCount)\n",
    "    #save the Term Document Matrix to CSV (so we can access it later)\n",
    "    np.savetxt(\"Features/\"+corp+\"/legitTermCount.csv\", legitTermCount, delimiter=\",\")\n",
    "    print(\"File Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all that pre-processing, now we can *finally* compute the mutual information score of the term on the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#A = P(X=x|C=c)\n",
    "#P(X=x|C=c) = (Total number of times x occurred in documents that are c)/(Total number of documents that are c)\n",
    "#B = P(X=x) = (Total number of times x occurred in the corpus)/(Total number of word occurrence in the corpus)\n",
    "#C = P(C=c) = (Total number of documents that are c in the corpus)/(Total number of documents in the corpus)\n",
    "      \n",
    "    \n",
    "def computeMI(termSpamCount, termLegitCount, totalWordOccurrence, totalLegitCount, totalSpamCount, totalDoc):\n",
    "    totalTermCount = termSpamCount + termLegitCount\n",
    "    \n",
    "    ASpam = (termSpamCount/totalSpamCount)\n",
    "    ALegit = (termLegitCount/totalLegitCount)\n",
    "    B = totalTermCount/totalWordOccurrence\n",
    "    CSpam = totalSpamCount/totalDoc\n",
    "    CLegit = totalLegitCount/totalDoc\n",
    "    \n",
    "    try:\n",
    "        try:\n",
    "            insideLog = (ASpam / (B*CSpam))\n",
    "        except ZeroDivisionError:\n",
    "            insideLog = 0\n",
    "        classSpam = ASpam * math.log10(insideLog)\n",
    "    except ValueError:\n",
    "        classSpam = 0\n",
    "    \n",
    "    try:\n",
    "        try:\n",
    "            insideLog = (ALegit / (B*CLegit))\n",
    "        except ZeroDivisionError:\n",
    "            insideLog = 0\n",
    "        classLegit = ALegit * math.log10(insideLog) \n",
    "    except ValueError:\n",
    "        classLegit = 0\n",
    "        \n",
    "    return (classSpam + classLegit) \n",
    "    \n",
    "\n",
    "def createMI(corp, totalLegitCount, totalSpamCount):\n",
    "    totalDoc  = totalLegitCount + totalSpamCount\n",
    "    \n",
    "    legitTermCountArr = genfromtxt('Features/'+corp+'/legitTermCount.csv', delimiter=',')\n",
    "    spamTermCountArr = genfromtxt('Features/'+corp+'/spamTermCount.csv', delimiter=',')\n",
    "    mi = []\n",
    "\n",
    "    vfunc = np.vectorize(computeMI)\n",
    "    totalWordOccurrence = np.sum(spamTermCountArr) + np.sum(legitTermCountArr) \n",
    "    mi = vfunc(spamTermCountArr, legitTermCountArr, totalWordOccurrence, totalLegitCount, totalSpamCount, totalDoc)\n",
    "\n",
    "    wordList = open(\"Features/\"+corp+\"/\"+corp+\"_words.csv\").read().splitlines()\n",
    "    print (corp, len(wordList), len(mi))\n",
    "    \n",
    "    termMI_List = pandas.DataFrame(\n",
    "        {'Term': wordList,\n",
    "         'MI': mi\n",
    "        })\n",
    "\n",
    "    pprint (termMI_List[:5])\n",
    "    sortedList =  termMI_List.sort_values('MI',ascending = False)\n",
    "    pprint (sortedList[:5])\n",
    "\n",
    "    #save the Term MI to CSV (so we can access it later)\n",
    "    sortedList.to_csv(\"Features/\"+corp+\"/\"+corp+\"termMI.csv\")\n",
    "    print(\"File Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that is all set, we can now start generating the data that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: preprocessCorpus  bare\n",
      "Done: preprocessCorpus  lemm\n",
      "Done: preprocessCorpus  lemm_stop\n",
      "Done: preprocessCorpus  stop\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# corpus list \n",
    "corpus = [\"bare\", \"lemm\",\"lemm_stop\", \"stop\"]\n",
    "\n",
    "#create a list that will hold CorpusData Object\n",
    "corpusDataList = []\n",
    "\n",
    "#you can execute the steps per corpus continuously, but since i have limited resources, I'll do it one at a time. \n",
    "#step1\n",
    "for corp in corpus:\n",
    "    corpusDataList.append(preprocessCorpus(corp))\n",
    "    print(\"Done: preprocessCorpus \", corp)\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The File that we are using for the documents is: Features/bare/bare_spamEmails.csv\n",
      "There are:  304 Spam Documents\n",
      "The File that we are using for the documents is: Features/bare/bare_legitEmails.csv\n",
      "There are:  2211 Legitimate Documents\n",
      "done\n",
      "Done: approach2  bare\n",
      "The File that we are using for the documents is: Features/lemm/lemm_spamEmails.csv\n",
      "There are:  452 Spam Documents\n",
      "The File that we are using for the documents is: Features/lemm/lemm_legitEmails.csv\n",
      "There are:  2324 Legitimate Documents\n",
      "done\n",
      "Done: approach2  lemm\n",
      "The File that we are using for the documents is: Features/lemm_stop/lemm_stop_spamEmails.csv\n",
      "There are:  281 Spam Documents\n",
      "The File that we are using for the documents is: Features/lemm_stop/lemm_stop_legitEmails.csv\n",
      "There are:  2409 Legitimate Documents\n",
      "done\n",
      "Done: approach2  lemm_stop\n",
      "The File that we are using for the documents is: Features/stop/stop_spamEmails.csv\n",
      "There are:  481 Spam Documents\n",
      "The File that we are using for the documents is: Features/stop/stop_legitEmails.csv\n",
      "There are:  1860 Legitimate Documents\n",
      "done\n",
      "Done: approach2  stop\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# corpus list \n",
    "corpus = [\"bare\", \"lemm\",\"lemm_stop\", \"stop\"]\n",
    "\n",
    "#step2\n",
    "for corp in corpus:\n",
    "    approach2(corp)\n",
    "    print(\"Done: approach2 \", corp)\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are:  47661  Words\n",
      "There are:  14488944  cells in the term document matrix\n",
      "There are:  105378471  cells in the term document matrix\n",
      "done\n",
      "Done: createTDM  bare\n",
      "There are:  44842  Words\n",
      "There are:  20268584  cells in the term document matrix\n",
      "There are:  104212808  cells in the term document matrix\n",
      "done\n",
      "Done: createTDM  lemm\n",
      "There are:  44228  Words\n",
      "There are:  12428068  cells in the term document matrix\n",
      "There are:  106545252  cells in the term document matrix\n",
      "done\n",
      "Done: createTDM  lemm_stop\n",
      "There are:  44749  Words\n",
      "There are:  21524269  cells in the term document matrix\n",
      "There are:  83233140  cells in the term document matrix\n",
      "done\n",
      "Done: createTDM  stop\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# corpus list \n",
    "corpus = [\"bare\", \"lemm\",\"lemm_stop\", \"stop\"]\n",
    "\n",
    "#step2\n",
    "for corp in corpus:\n",
    "    createTDM(corp)\n",
    "    print(\"Done: createTDM \",corp)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam TDM :  (304, 47661)\n",
      "done\n",
      "File saved\n",
      "Done: processSpamTDM  bare\n",
      "Legit TDM :  (2211, 47661)\n",
      "done\n",
      "File Saved\n",
      "Done: processLegitTDM  bare\n",
      "Spam TDM :  (452, 44842)\n",
      "done\n",
      "File saved\n",
      "Done: processSpamTDM  lemm\n",
      "Legit TDM :  (2324, 44842)\n",
      "done\n",
      "File Saved\n",
      "Done: processLegitTDM  lemm\n",
      "Spam TDM :  (281, 44228)\n",
      "done\n",
      "File saved\n",
      "Done: processSpamTDM  lemm_stop\n",
      "Legit TDM :  (2409, 44228)\n",
      "done\n",
      "File Saved\n",
      "Done: processLegitTDM  lemm_stop\n",
      "Spam TDM :  (481, 44749)\n",
      "done\n",
      "File saved\n",
      "Done: processSpamTDM  stop\n",
      "Legit TDM :  (1860, 44749)\n",
      "done\n",
      "File Saved\n",
      "Done: processLegitTDM  stop\n"
     ]
    }
   ],
   "source": [
    "# corpus list \n",
    "corpus = [\"bare\", \"lemm\",\"lemm_stop\", \"stop\"]\n",
    "\n",
    "for corp in corpus:    \n",
    "    processSpamTDM(corp)\n",
    "    print(\"Done: processSpamTDM \",corp)\n",
    "    processLegitTDM(corp)\n",
    "    print(\"Done: processLegitTDM \",corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i will populate corpusDataList MANUALLY for now, because i already know how many are the legit emails and spam emails are per corpus but in reality you could:\n",
    "1. Execute preprocessing (but why)\n",
    "2. Create a new function that counts how many legit and spam emails are in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bare 47661 47661\n",
      "         MI          Term\n",
      "0  0.014192    succession\n",
      "1  0.290927         india\n",
      "2  0.023367         loser\n",
      "3  0.007096  proscriptive\n",
      "4  0.007096    bonvillain\n",
      "              MI     Term\n",
      "5354   18.049254  subject\n",
      "40233  11.429276     this\n",
      "21546  11.349580     with\n",
      "8553   10.438905     from\n",
      "8126   10.261849     that\n",
      "File Saved\n",
      "Done: createMI  bare\n",
      "lemm 44842 44842\n",
      "         MI          Term\n",
      "0  0.002032    succession\n",
      "1  0.001016         india\n",
      "2  0.025039         loser\n",
      "3  0.001016  proscriptive\n",
      "4  0.001016         celao\n",
      "             MI     Term\n",
      "4967   5.279136  subject\n",
      "37863  3.734168     this\n",
      "4194   3.511598     have\n",
      "20222  3.460249     with\n",
      "7943   3.244249     from\n",
      "File Saved\n",
      "Done: createMI  lemm\n",
      "lemm_stop 44228 44228\n",
      "         MI          Term\n",
      "0  0.001883    succession\n",
      "1  0.000941         india\n",
      "2  0.043492         loser\n",
      "3  0.000941  proscriptive\n",
      "4  0.000941         celao\n",
      "             MI         Term\n",
      "4896   5.372569      subject\n",
      "24169  2.778434         mail\n",
      "14508  2.502427       please\n",
      "16089  2.447028  information\n",
      "3967   2.342945         free\n",
      "File Saved\n",
      "Done: createMI  lemm_stop\n",
      "stop 44749 44749\n",
      "         MI          Term\n",
      "0  0.002576    succession\n",
      "1  0.038635         india\n",
      "2  0.015144         loser\n",
      "3  0.001288  proscriptive\n",
      "4  0.001288    bonvillain\n",
      "             MI         Term\n",
      "5023   5.178360      subject\n",
      "14749  2.381570       please\n",
      "16316  2.223589  information\n",
      "4108   2.101506         free\n",
      "24534  2.088032         mail\n",
      "File Saved\n",
      "Done: createMI  stop\n"
     ]
    }
   ],
   "source": [
    "#create a list that will hold CorpusData Object\n",
    "corpusDataList = []\n",
    "\n",
    "bare = CorpusData(\"bare\", 2515, 304, 452)\n",
    "lemm = CorpusData(\"lemm\", 2776, 452, 2324)\n",
    "lemm_stop = CorpusData(\"lemm_stop\", 2609, 281, 2409)\n",
    "stop = CorpusData(\"stop\", 2341, 481, 1860)\n",
    "\n",
    "corpusDataList.append(bare)\n",
    "corpusDataList.append(lemm)\n",
    "corpusDataList.append(lemm_stop)\n",
    "corpusDataList.append(stop)\n",
    "\n",
    "for corp in corpusDataList:  \n",
    "    createMI(corp.corpusName, corp.legitEmailCtr, corp.spamEmailCtr)\n",
    "    print(\"Done: createMI \",corp.corpusName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Check the features folder if you want to see the generated files, this is the end of the feature extraction tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
