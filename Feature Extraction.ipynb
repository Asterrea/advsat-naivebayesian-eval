{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The features that will be used are the words that can be found in the each document in the entire corpus. In order to extract all the features from the corpus, we first need to extract all the words in the corpus, the words will serve as a feature\n",
    "\n",
    "The code below, extracts the words in a textfle given its filepath\n",
    "\n",
    "***words =  [i for i in words if len(i) >= 3]*** filters out the words that are less than 3 letters\n",
    "\n",
    "*** words = list(set(words))*** Creates a set from the list of words extracted, and duplicate words are removed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "def extractWords(filepath):\n",
    "    file = open(filepath, 'r')\n",
    "    # .lower() returns a version with all upper case characters replaced with lower case characters.\n",
    "    text = file.read().lower()\n",
    "    file.close()\n",
    "    # replaces anything that is not a lowercase letter, a space, or an apostrophe with a space:\n",
    "    text = re.sub('[^a-z]+', \" \", text)\n",
    "    words = list(text.split())\n",
    "    \n",
    "     # remove duplicate words in the list\n",
    "    words = list(set(words))\n",
    "    # removes words that are less than 3 letters/characters\n",
    "    words =  [i for i in words if len(i) >= 3] \n",
    "    return words;\n",
    "\n",
    "# this functions write the list of tokens in the tokenWords parameter on the filepath passed on filename parameter \n",
    "def writeToFile(filename, tokenWords):\n",
    "    f = open(filename, 'w')\n",
    "    json.dump(tokenWords, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below traverses all of the documents that is under a given directory, we have 4 corpus, so we need to do this 4 times, one for each corpus. \n",
    "\n",
    "These are the 4 corpus\n",
    "1. Bare : Original Email Data\n",
    "2. Lemm : Words in the emails are lemmatized\n",
    "3. Stop : Stops words are removed from the emails\n",
    "4. Lemm_stop : Words are lemmatized and stop words are removed from the emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "tokenList = []\n",
    "corpus = [\"bare\", \"lemm\", \"lemm_stop\", \"stop\"]\n",
    "fileCtr = 0\n",
    "\n",
    "for corp in corpus:\n",
    "    rootdir = \"Emails\\\\\"\n",
    "    rootdir = rootdir + corp\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            tempList = []\n",
    "            filepath =  subdir + os.sep + file\n",
    "\n",
    "            tempList = extractWords(filepath)\n",
    "            tokenList.extend(tempList)\n",
    "            fileCtr = fileCtr + 1\n",
    "            \n",
    "    tokenList = list(set(tokenList))     \n",
    "    # print (tokenList)\n",
    "\n",
    "    #tokens/words extracted from the emails will be stored in a text file so it can be used for further analyzation\n",
    "    writeToFile(\"Features\\\\\"+corp+\"_words.txt\", tokenList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 2D matrix of term occurrence, x = terms, y = documents, 1 = term occurred in document then 0 otherwise, \n",
    "\n",
    "Pattern  = d+-\\d+msg\\d+.txt, only matches documents that are NOT spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bare 2515 304 2211\n",
      "lemm 2776 452 2324\n",
      "lemm_stop 2690 281 2409\n",
      "stop 2341 481 1860\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# create a class for the the Corpus data, it will store the total number of emails in the corpus, \n",
    "# along with the total number of spam and legit emails\n",
    "class CorpusData: \n",
    "    corpusName = \"\"\n",
    "    totalEmails = 0\n",
    "    spamEmails = 0\n",
    "    legitEmails = 0\n",
    "\n",
    "    def __init__(self, corpusName, totalEmails, spamEmails, legitEmails):\n",
    "        self.corpusName = corpusName\n",
    "        self.totalEmails = totalEmails\n",
    "        self.spamEmails = spamEmails\n",
    "        self.legitEmails = legitEmails\n",
    "\n",
    "#create a list that will hold CorpusData Object\n",
    "corpusDataList = []\n",
    "\n",
    "# this will give us the number of documents there are for each corpus\n",
    "corpus = [\"bare\", \"lemm\",\"lemm_stop\", \"stop\"]\n",
    "counter = 0\n",
    "legitEmailCTR = 0\n",
    "spamCTR = 0\n",
    "pattern = re.compile(\"\\d+-\\d+msg\\d+.txt\")\n",
    "for corp in corpus: \n",
    "    rootdir = \"Emails\\\\\"\n",
    "    rootdir = rootdir + corp\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            counter = counter +1\n",
    "            if pattern.match(file):\n",
    "                legitEmailCTR =  legitEmailCTR + 1\n",
    "            else: \n",
    "                spamCTR = spamCTR + 1\n",
    "    \n",
    "    x = CorpusData(corp, counter, spamCTR, legitEmailCTR)\n",
    "    corpusDataList.append(x)\n",
    "    \n",
    "    legitEmailCTR = 0\n",
    "    spamCTR = 0\n",
    "    counter = 0  \n",
    "    \n",
    "for x in corpusDataList:\n",
    "    print (x.corpusName, x.totalEmails, x.spamEmails, x.legitEmails)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 2d vector, x = terms, y = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#loads all the words extracted in the 'bare' corpus / dataset\n",
    "file = open(\"Features\\\\bare_words.txt\", 'r')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "text = re.sub('[^a-z]+', \" \", text)\n",
    "words = list(text.split())\n",
    "\n",
    "#termCount = total number of words in the corpus\n",
    "termCount = len(words)\n",
    "#totalLegitDocuments = total number of LEGIT Emails in the corpus\n",
    "totalLegitDocuments = corpusDataList[0].legitEmails\n",
    "\n",
    "# print (totalLegitDocuments)\n",
    "# print (\"termCount: \", termCount)\n",
    "\n",
    "#this will create a 2D vector that will be used to store the term document matrix\n",
    "tdm = np.zeros((termCount, totalLegitDocuments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if  \"grote\" in words: \n",
    "    b=words.index(\"grote\")\n",
    "    print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenList = []\n",
    "# rootdir = \"Emails\\\\bare\"\n",
    "# fileCtr = 0\n",
    "# pattern = re.compile(\"\\d+-\\d+msg\\d+.txt\")\n",
    "# for subdir, dirs, files in os.walk(rootdir):\n",
    "#     for file in files:\n",
    "#         tempList = []\n",
    "#         #print os.path.join(subdir, file)\n",
    "#         filepath =  subdir + os.sep + file\n",
    "        \n",
    "#         if pattern.match(file):   \n",
    "#             fileCtr = fileCtr + 1\n",
    "            \n",
    "            \n",
    "# #Prints the cleaned tokenList\n",
    "# tokenList = list(set(tokenList))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
