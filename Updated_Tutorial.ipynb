{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Tutorial based from the paper : \"An Evaluation of Naive Bayesian Anti-Spam Filtering\n",
    "\n",
    "This is a tutorial to replicate the process of the paper entitled: \"An Evaluation of Naive Bayesian Anti-Spam Filtering. The objective of the paper is to filter spam emails using a Naive Bayes Techinique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our Data\n",
    "\n",
    "The dataset provided to us has 4 directories [bare, lemm, lemm_stop, stop] with subdirectories of 10 parts. Where 9 were used as training set and 1 reserved for testing for every repetition. To reduce random variation, a ten cross-validation was done yielding the ten subdirectories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To form a list of features to finally use in predicting classification using the Naive Bayes theorem, a common feature selection method is done by computing the Mutual Information (MI) of term t and class c \n",
    "\n",
    "where, \n",
    "- <i>t</i> is defined to be a word attribute and;\n",
    "- classified either as <i>c</i> = spam or not spam. \n",
    "\n",
    "We are given a description <i>d âˆˆ X</i> of a document, where <i>X</i> is the document space; and a fixed set of classes <i>C = {spam, legitimate}</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training our data to filter out spam, we need to find the features that will be used, the paper used the words found in the corpus as the features of the classifier. We created a whole tutorial on feature extraction alone, refer to another notebook named: <b><i>Jupyter Feature Extraction</i></b>. \n",
    "\n",
    "After reading the feature extraction tutorial, you already have the list of all the terms in each corpus with their corresponding Mutual Information score, the following function will extract n-terms with the highest <i>MI</i>. The you/system decide how many features will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "numMI = 10\n",
    "corpus = ['bare','lemm', 'lemm_stop', 'stop']\n",
    "\n",
    "for corp in corpus:\n",
    "    termMIList = pd.read_csv(\"Features/\"+corp+\"/\"+corp+\"termMI.csv\", index_col = 0)\n",
    "    terms = pd.DataFrame(termMIList['Term'].head(n=numMI).tolist()).to_csv(\"MI/\"+corp+\"MI.txt\", \n",
    "                                                                    header = None, index = None)\n",
    "print (\"Done, check MI folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the train and test set split by passing the extracted <i>.txt</i> file to the function below that automatically walks through the directory given to it and transfers the contents of the text file into a list called <i>dir_dataset</i>, where the function <i>parse_subdirectories</i> returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# eg. passed directory = 'Emails/bare'\n",
    "# reads its subdirectories and files into a list of lists\n",
    "def parse_subdirectories(directory):\n",
    "    for path, subdirs, files in os.walk(directory):\n",
    "        dir_dataset = []\n",
    "        for filename in files:\n",
    "            f = os.path.join(path,filename)\n",
    "            subdir_content = []\n",
    "            with open(f,'r') as file_content:\n",
    "                content = file_content.read()\n",
    "                subdir_content.append(content)\n",
    "        dir_dataset.append(subdir_content)\n",
    "    return dir_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split this list data into 90% training set and 10% data for accuracy testing of Naive Bayes prediction. Let's call these list variables: <i>train_set</i> and <i>test_set</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_dataset(dataset, n_split):\n",
    "    train_set = []\n",
    "    data_copy = list(dataset)\n",
    "    while len(train_set) < int(len(dataset)*n_split):\n",
    "        pointer = random.randrange(len(data_copy))\n",
    "        train_set.append(data_copy.pop(pointer))\n",
    "    return [train_set,data_copy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Email/Documents\n",
    "\n",
    "To start classifying if the instance of document <i>X</i> is a legitimate message or spam, we first create a <b>Term Matrix</b>. The term matrix contains the final features cross-checked in every document if it exists or not denoted by: <i>0 or 1</i>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "features= []\n",
    "\n",
    "#MIfilepath: 'MI/[filename].txt\n",
    "def readFeatures(MIfilepath):\n",
    "    with open(MIfilepath) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.rstrip()\n",
    "            features.append(line)\n",
    "    f.close()\n",
    "    \n",
    "#corpusdirectory: 'Emails/bare'\n",
    "#corpusname : bare\n",
    "def build_term_matrix(corpusdirectory, corpusname):\n",
    "    directory = os.path.dirname(\"Term Matrix/\")\n",
    "    subdir = os.path.join(directory,corpusname)\n",
    "    if not os.path.exists(subdir):\n",
    "        os.makedirs(subdir)\n",
    "    csvpath = os.path.join(subdir,corpusname)\n",
    "    csvfile = open(csvpath + '.csv', 'w')\n",
    "    csv_writer = csv.writer(csvfile,delimiter=',')\n",
    "    \n",
    "    for dirs,subdirs,files in os.walk(corpusdirectory):\n",
    "        for messages in sorted(files):\n",
    "            f = os.path.join(dirs,messages)\n",
    "            with open(f,'r') as email:\n",
    "                content = email.read().split(' ')\n",
    "                csv_writer.writerow([messages])\n",
    "                output = []\n",
    "                for feature in features:\n",
    "                    r = csv.reader(open(csvpath+'.csv'))\n",
    "                    if feature in content:\n",
    "                        if os.path.exists(csvpath+'.csv'):\n",
    "                            for row in r:\n",
    "                                row.append('1')\n",
    "                                output.append(row)\n",
    "                    else:\n",
    "                        if os.path.exists(csvpath+'.csv'):\n",
    "                            for row in r:\n",
    "                                row.append('0')\n",
    "                                output.append(row)\n",
    "                csv_writer.writerows(output)\n",
    "                print(output)\n",
    "            email.close()\n",
    "    csvfile.close()\n",
    "        \n",
    "readFeatures('MI/bareMI.txt')\n",
    "build_term_matrix('Emails/bare', 'bare')\n",
    "readFeatures('MI/lemmMI.txt')\n",
    "build_term_matrix('Emails/lemm', 'lemm')\n",
    "readFeatures('MI/lemm_stopMI.txt')\n",
    "build_term_matrix('Emails/lemm_stop', 'lemm_stop')\n",
    "readFeatures('MI/stopMI.txt')\n",
    "build_term_matrix('Emails/stop','stop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The classifier we will be building will use \"supervised learning\". Now that we have the a list of features, its association with all messages using the term matrix, this will form the basis of the classifier.\n",
    "\n",
    "Once we have associated the various words with our two classifications (spam and legit), we can calculate the probability that a given word belongs to either spam or legit category. For instance, the probability that the word \"vintage\" appears in a spam message is much higher than the probability it appears in a legitimate email.\n",
    "\n",
    "For example, once we have trained our classifier using 200 documents, 100 are spam and 100 are legit. If word \"vintage\" appears in 25 spam documents, but only 5 legit documents. The probability, then, that the word \"vintage\" classifies as a spam document is calculated:\n",
    "\n",
    "$$P(\"vintage\" | spam) = (.25 * .5) / ((.25 * .5) + (.05 * .5)) = .83, or 83%.$$\n",
    "\n",
    "The \".25\" and \".05\" are the percentage of documents containing the word money that are spam and ham respectively. The \".5\" is the interesting number and is the percentage of documents that are spam or legit. Since we have classified 100 of each, the total number of documents is 200, and it is overall 50% likely that a document is spam.\n",
    "\n",
    "By combining the probabilities for all the words in a document, it is possible to get an overall view of the likelihood a document is either spam or legit.\n",
    "\n",
    "Let us break this down for a while, we need the following count:\n",
    "- Count of documents\n",
    "- Actual count of legit and spam documents\n",
    "- Feature count of words on all documents segregated between the categories (legit/spam)\n",
    "\n",
    "Let's create function/s for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(289, 48, 241)\n",
      "(279, 48, 231)\n",
      "(289, 48, 241)\n",
      "(289, 48, 241)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#subdirectory : 'Emails/bare\n",
    "def countDocuments(directory):\n",
    "    for path, subdirs, files in os.walk(directory):\n",
    "        for subdir in subdirs:\n",
    "            dir_path = os.path.join(path,subdir)\n",
    "            docCount = len([name for name in os.listdir(dir_path)])\n",
    "            spamCount = 0\n",
    "            legitCount = 0\n",
    "            for name in os.listdir(dir_path):\n",
    "                if name.startswith(\"spm\"):\n",
    "                    spamCount += 1\n",
    "                else:\n",
    "                    legitCount += 1\n",
    "    return docCount,spamCount,legitCount\n",
    "\n",
    "print(countDocuments('Emails/bare'))\n",
    "print(countDocuments('Emails/lemm'))\n",
    "print(countDocuments('Emails/lemm_stop'))\n",
    "print(countDocuments('Emails/stop'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon constructing the <b>Term Matrix</b> we can now proceed in computing for the probability of classifiying it as a spam or not with the equation:\n",
    "\n",
    "$$\\frac{P(C=spam|\\vec{X} = \\vec{x})}{P(C=legitimate | \\vec{X} = \\vec{x})} > \\lambda$$\n",
    "\n",
    "The equation tells us that we need the vectors from the term matrix which indicates association between the features and documents in order to assess if it is a spam or not. Therefore we can do it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def probability_feature():\n",
    "def weighted_probability():\n",
    "def probability_document():\n",
    "def final_probability():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pass each to train, classify and test : dataset\n",
    "bare_dataset = parse_subdirectories('Emails/bare')\n",
    "lemm_dataset = parse_subdirectories('Emails/lemm')\n",
    "lemm_stop_dataset = parse_subdirectories('Emails/lemm_stop')\n",
    "stop_dataset = parse_subdirectories('Emails/stop')\n",
    "\n",
    "def train(dataset, threshold):\n",
    "    train_set, test_set = split_dataset(dataset)\n",
    "    #ready csv file to write\n",
    "    #in term matrix directory : do file walk\n",
    "        #read term matrix : for every row get vectors\n",
    "        #compute predicted result\n",
    "        #create row : store filename , actual  , predicted\n",
    "def classify():\n",
    "    \n",
    "def test(dataset):\n",
    "    train_set, test_set = split_dataset(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
