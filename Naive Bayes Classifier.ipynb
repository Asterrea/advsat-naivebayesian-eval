{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check feature extraction tutorial first before this. \n",
    "\n",
    "After feature extraction, it will generate a csv file that has:\n",
    "1. Term\n",
    "2. Number of legitimate emails that word occurred in\n",
    "3. Number of spam emails that word occurred in\n",
    "4. Mutual Information Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing the mutual information score of each term, we can now extract the top n highest terms based on the MI to be used as the features for the classifier. The paper used 50-700 features, step 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, check MI folder\n"
     ]
    }
   ],
   "source": [
    "numMI = [50,100,150,200,250,300,350,400,450,500,550,600,650,700]\n",
    "corpus = ['bare','lemm', 'lemm_stop', 'stop']\n",
    "\n",
    "for corp in corpus:\n",
    "    for num in numMI:\n",
    "        termMIList = pd.read_csv(\"Features/\"+corp+\"/\"+corp+\"termMI.csv\", index_col = 0)\n",
    "        terms = pd.DataFrame(termMIList.head(n=num)).to_csv(\"MI/\"+corp+\"/\"+str(num)+\"terms.csv\")\n",
    "print (\"Done, check MI folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to classify, insert formula hereeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from decimal import *\n",
    "\n",
    "getcontext().prec = 256\n",
    "getcontext().rounding = ROUND_UP\n",
    "\n",
    "# A = P(X=x|C=c)\n",
    "# P(A|B), is equal to P(AB)/P(B).\n",
    "# P(A) = (Total number of times x occurred/total number of term occurrence in the corpus)\n",
    "# P(B) = (Total number of email c in the corpus /total number of documents in the corpus)\n",
    "#termProb = total number of times x occurred  in document\n",
    "\n",
    "\n",
    "def computeTermGivenClass(probClass, terms, totalTerms):\n",
    "    prob = Decimal(1.0)\n",
    "    for x in terms:\n",
    "        a = (Decimal(x)/Decimal(totalTerms))\n",
    "        prob = (Decimal(prob) * (Decimal(a)/Decimal(probClass)))\n",
    "    \n",
    "    return prob\n",
    "\n",
    "#B = P(X=x) = (Total number of times x occurred in the corpus)/(Total number of word occurrence in the corpus)\n",
    "#C = P(C=c) = (Total number of documents that are c in the corpus)/(Total number of documents in the corpus)\n",
    "          \n",
    "def computeProbability(isComputingSpam, totalDoc, spamTerms, legitTerms, totalTerms, totalLegitCount, totalSpamCount):\n",
    "      \n",
    "    probSpam = (totalSpamCount/totalDoc)\n",
    "    probLegit = (totalLegitCount/totalDoc)\n",
    "    \n",
    "    givenSpam =  (Decimal(probSpam) * computeTermGivenClass(probSpam, spamTerms, totalTerms))\n",
    "    givenLegit = (Decimal(probLegit) * computeTermGivenClass(probLegit, legitTerms, totalTerms))\n",
    "    \n",
    "    if isComputingSpam == True:\n",
    "        numerator =  givenSpam\n",
    "    else:\n",
    "        numerator =  givenLegit\n",
    "            \n",
    "    denominator = Decimal(givenSpam) + Decimal(givenLegit)\n",
    "    try:\n",
    "        probClass = Decimal(numerator/denominator)\n",
    "    except:\n",
    "        probClass = 0\n",
    "    return probClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def extractWords(filepath):\n",
    "    file = open(filepath, 'r')\n",
    "    # .lower() returns a version with all upper case characters replaced with lower case characters.\n",
    "    text = file.read().lower()\n",
    "    file.close()\n",
    "    # replaces anything that is not a lowercase letter, a space, or an apostrophe with a space:\n",
    "    text = re.sub('[^a-z]+', \" \", text)\n",
    "    words = list(text.split())\n",
    "    \n",
    "     # remove duplicate words in the list\n",
    "    words = list(set(words))\n",
    "    # removes words that are less than 4 letters/characters\n",
    "    words =  [i for i in words if len(i) >= 4] \n",
    "    return words;\n",
    "\n",
    "def findTermCounts(terms, docTerms):\n",
    "    return terms.loc[terms['Term'].isin(docTerms)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classifyDocs(corp, terms, spamCount, legitCount, totalTerms,\n",
    "             fileList, threshold, numFeatures):\n",
    "    \n",
    "    totalDoc = corp.totalEmailCtr\n",
    "    totalLegitCount = corp.legitEmailCtr\n",
    "    totalSpamCount = corp.spamEmailCtr\n",
    "\n",
    "    predicted = []\n",
    "    for filepath in fileList:   \n",
    "\n",
    "        docTerms = extractWords(filepath)\n",
    "        rowTerms = findTermCounts(terms, docTerms)\n",
    "        PCSpam = Decimal(computeProbability(True, totalDoc, spamCount, legitCount, \n",
    "                                            totalTerms, totalLegitCount, totalSpamCount))\n",
    "        PCLegit = Decimal(computeProbability(False, totalDoc, spamCount, legitCount, \n",
    "                                            totalTerms, totalLegitCount, totalSpamCount))\n",
    "        \n",
    "        try:\n",
    "            ifSpam = PCSpam/PCLegit\n",
    "        except:\n",
    "            ifSpam = 0\n",
    "            \n",
    "        if  ifSpam > threshold:\n",
    "            predicted.append(0)\n",
    "        else:\n",
    "            predicted.append(1)\n",
    "    \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a class for the the Corpus data, it will store the total number of emails in the corpus, \n",
    "# along with the total number of spam and legit emails\n",
    "class CorpusData: \n",
    "    corpusName = \"\"\n",
    "    totalEmailCtr = 0\n",
    "    spamEmailCtr = 0\n",
    "    legitEmailCtr = 0\n",
    "\n",
    "    def __init__(self, corpusName, totalEmailCtr, spamEmailCtr, legitEmailCtr):\n",
    "        self.corpusName = corpusName\n",
    "        self.totalEmailCtr = totalEmailCtr\n",
    "        self.spamEmailCtr = spamEmailCtr\n",
    "        self.legitEmailCtr = legitEmailCtr   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combineFiles(corp):\n",
    "    #for each subdirectory in a corpus (folders - part 1 - 10)\n",
    "    fileList = []\n",
    "    rootdir = \"Emails/\"+corp\n",
    "    actualClass = []\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "    #for each file in a folder\n",
    "        for file in files:\n",
    "            filepath = subdir + \"/\" + file\n",
    "            fileList.append(filepath)\n",
    "            \n",
    "            if pattern.match(file): \n",
    "                    actualClass.append(1)\n",
    "            else:\n",
    "                    actualClass.append(0)\n",
    "    \n",
    "    return fileList, actualClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeResults(actual, predicted, corp, featureNum, thresh):\n",
    "    results = pd.DataFrame(\n",
    "        {'Actual': actual,\n",
    "         'Predicted' : predicted,\n",
    "        })\n",
    "\n",
    "    #save the Term MI to CSV (so we can access it later)\n",
    "    results.to_csv(\"Classified/\"+corp.corpusName +\"/\"+str(featureNum)+\"/\"+str(thresh)+\"_results.csv\")\n",
    "    print(\"File Saved: \", corp.corpusName , featureNum, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total emails, total spam emails, total legit emails\n",
    "bare = CorpusData(\"bare\", 2515, 304, 2211)\n",
    "lemm = CorpusData(\"lemm\", 2776, 452, 2324)\n",
    "lemm_stop = CorpusData(\"lemm_stop\", 2609, 281, 2409)\n",
    "stop = CorpusData(\"stop\", 2341, 481, 1860)\n",
    "\n",
    "\n",
    "numFeatures = [50,100,150,200,250,300,350,400,450,500,550,600,650,700]\n",
    "# 0.5 - 1, 0.9 - 9, 0.999 - 999\n",
    "threshold = [0.5, 0.9, 0.999]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(corp):\n",
    "    print (\"Classifying\", corp.corpusName)\n",
    "    #load the vocabulary/word/term list for the entire corpus from file\n",
    "    corpusTerms = pd.read_csv(\"Features/\"+corp.corpusName+\"/\"+corp.corpusName+\"termMI.csv\", index_col = 0)\n",
    "    totalLegitTerms = corpusTerms['LegitCount'].sum(axis=0)\n",
    "    totalSpamTerms = corpusTerms['SpamCount'].sum(axis=0)\n",
    "    totalTerms = totalLegitTerms + totalSpamTerms\n",
    "    \n",
    "    filepathList, actualClass = combineFiles(\"bare\")\n",
    "    for num in numFeatures:  \n",
    "        terms = pd.read_csv(\"MI/\"+corp.corpusName+\"/\"+str(num)+\"terms.csv\", index_col = 0)\n",
    "        print (\"Features: \", num) \n",
    "        \n",
    "        spamCount = terms['SpamCount'].tolist()\n",
    "        legitCount = terms['LegitCount'].tolist()\n",
    "        \n",
    "        for t in threshold:\n",
    "            print (\"Threshold: \", t)\n",
    "            predClass = classifyDocs(corp, terms, spamCount, legitCount, totalTerms,\n",
    "                                 filepathList, t, num)\n",
    "            writeResults(actualClass,predClass, corp, num, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying bare\n",
      "Features:  50\n",
      "Threshold:  0.5\n",
      "File Saved:  bare 50 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  bare 50 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  bare 50 0.999\n",
      "Features:  100\n",
      "Threshold:  0.5\n",
      "File Saved:  bare 100 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  bare 100 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  bare 100 0.999\n",
      "Features:  150\n",
      "Threshold:  0.5\n",
      "File Saved:  bare 150 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  bare 150 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  bare 150 0.999\n",
      "Features:  200\n",
      "Threshold:  0.5\n",
      "File Saved:  bare 200 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  bare 200 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  bare 200 0.999\n",
      "Features:  250\n",
      "Threshold:  0.5\n",
      "File Saved:  bare 250 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  bare 250 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  bare 250 0.999\n",
      "Features:  300\n",
      "Threshold:  0.5\n",
      "File Saved:  bare 300 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  bare 300 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  bare 300 0.999\n",
      "Features:  350\n",
      "Threshold:  0.5\n",
      "File Saved:  bare 350 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  bare 350 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  bare 350 0.999\n",
      "Features:  400\n",
      "Threshold:  0.5\n"
     ]
    }
   ],
   "source": [
    "classify(bare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying bare\n",
      "Features:  400\n",
      "Threshold:  0.5\n",
      "File Saved:  bare 400 0.5\n"
     ]
    }
   ],
   "source": [
    "print (\"Classifying\", bare.corpusName)\n",
    "#load the vocabulary/word/term list for the entire corpus from file\n",
    "corpusTerms = pd.read_csv(\"Features/\"+bare.corpusName+\"/\"+bare.corpusName+\"termMI.csv\", index_col = 0)\n",
    "totalLegitTerms = corpusTerms['LegitCount'].sum(axis=0)\n",
    "totalSpamTerms = corpusTerms['SpamCount'].sum(axis=0)\n",
    "totalTerms = totalLegitTerms + totalSpamTerms\n",
    "\n",
    "filepathList, actualClass = combineFiles(\"bare\")\n",
    "num = 400 \n",
    "terms = pd.read_csv(\"MI/\"+bare.corpusName+\"/\"+str(num)+\"terms.csv\", index_col = 0)\n",
    "print (\"Features: \", num) \n",
    "\n",
    "spamCount = terms['SpamCount'].tolist()\n",
    "legitCount = terms['LegitCount'].tolist()\n",
    "\n",
    "t = 0.5\n",
    "print (\"Threshold: \", t)\n",
    "predClass = classifyDocs(bare, terms, spamCount, legitCount, totalTerms,\n",
    "                     filepathList, t, num)\n",
    "writeResults(actualClass,predClass, bare, num, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying lemm\n",
      "Features:  50\n",
      "Threshold:  0.5\n",
      "File Saved:  lemm 50 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  lemm 50 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  lemm 50 0.999\n",
      "Features:  100\n",
      "Threshold:  0.5\n",
      "File Saved:  lemm 100 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  lemm 100 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  lemm 100 0.999\n",
      "Features:  150\n",
      "Threshold:  0.5\n",
      "File Saved:  lemm 150 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  lemm 150 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  lemm 150 0.999\n",
      "Features:  200\n",
      "Threshold:  0.5\n",
      "File Saved:  lemm 200 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  lemm 200 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  lemm 200 0.999\n",
      "Features:  250\n",
      "Threshold:  0.5\n",
      "File Saved:  lemm 250 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  lemm 250 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  lemm 250 0.999\n",
      "Features:  300\n",
      "Threshold:  0.5\n",
      "File Saved:  lemm 300 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  lemm 300 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  lemm 300 0.999\n",
      "Features:  350\n",
      "Threshold:  0.5\n",
      "File Saved:  lemm 350 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  lemm 350 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  lemm 350 0.999\n",
      "Features:  400\n",
      "Threshold:  0.5\n",
      "File Saved:  lemm 400 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  lemm 400 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  lemm 400 0.999\n",
      "Features:  450\n",
      "Threshold:  0.5\n",
      "File Saved:  lemm 450 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  lemm 450 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  lemm 450 0.999\n",
      "Features:  500\n",
      "Threshold:  0.5\n",
      "File Saved:  lemm 500 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  lemm 500 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  lemm 500 0.999\n",
      "Features:  550\n",
      "Threshold:  0.5\n",
      "File Saved:  lemm 550 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  lemm 550 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  lemm 550 0.999\n",
      "Features:  600\n",
      "Threshold:  0.5\n",
      "File Saved:  lemm 600 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  lemm 600 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  lemm 600 0.999\n",
      "Features:  650\n",
      "Threshold:  0.5\n",
      "File Saved:  lemm 650 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  lemm 650 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  lemm 650 0.999\n",
      "Features:  700\n",
      "Threshold:  0.5\n",
      "File Saved:  lemm 700 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  lemm 700 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  lemm 700 0.999\n"
     ]
    }
   ],
   "source": [
    "classify(lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying stop\n",
      "Features:  50\n",
      "Threshold:  0.5\n",
      "File Saved:  stop 50 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  stop 50 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  stop 50 0.999\n",
      "Features:  100\n",
      "Threshold:  0.5\n",
      "File Saved:  stop 100 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  stop 100 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  stop 100 0.999\n",
      "Features:  150\n",
      "Threshold:  0.5\n",
      "File Saved:  stop 150 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  stop 150 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  stop 150 0.999\n",
      "Features:  200\n",
      "Threshold:  0.5\n",
      "File Saved:  stop 200 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  stop 200 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  stop 200 0.999\n",
      "Features:  250\n",
      "Threshold:  0.5\n",
      "File Saved:  stop 250 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  stop 250 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  stop 250 0.999\n",
      "Features:  300\n",
      "Threshold:  0.5\n",
      "File Saved:  stop 300 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  stop 300 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  stop 300 0.999\n",
      "Features:  350\n",
      "Threshold:  0.5\n",
      "File Saved:  stop 350 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  stop 350 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  stop 350 0.999\n",
      "Features:  400\n",
      "Threshold:  0.5\n",
      "File Saved:  stop 400 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  stop 400 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  stop 400 0.999\n",
      "Features:  450\n",
      "Threshold:  0.5\n",
      "File Saved:  stop 450 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  stop 450 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  stop 450 0.999\n",
      "Features:  500\n",
      "Threshold:  0.5\n",
      "File Saved:  stop 500 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  stop 500 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  stop 500 0.999\n",
      "Features:  550\n",
      "Threshold:  0.5\n",
      "File Saved:  stop 550 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  stop 550 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  stop 550 0.999\n",
      "Features:  600\n",
      "Threshold:  0.5\n",
      "File Saved:  stop 600 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  stop 600 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  stop 600 0.999\n",
      "Features:  650\n",
      "Threshold:  0.5\n",
      "File Saved:  stop 650 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  stop 650 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  stop 650 0.999\n",
      "Features:  700\n",
      "Threshold:  0.5\n",
      "File Saved:  stop 700 0.5\n",
      "Threshold:  0.9\n",
      "File Saved:  stop 700 0.9\n",
      "Threshold:  0.999\n",
      "File Saved:  stop 700 0.999\n"
     ]
    }
   ],
   "source": [
    "classify(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Classifying lemm_stop\n",
       "Features:  50\n",
       "Threshold:  0.5\n",
       "File Saved:  stop 50 0.5\n",
       "Threshold:  0.9\n",
       "File Saved:  stop 50 0.9\n",
       "Threshold:  0.999\n",
       "File Saved:  stop 50 0.999\n",
       "Features:  100\n",
       "Threshold:  0.5\n",
       "File Saved:  stop 100 0.5\n",
       "Threshold:  0.9\n",
       "File Saved:  stop 100 0.9\n",
       "Threshold:  0.999\n",
       "File Saved:  stop 100 0.999\n",
       "Features:  150\n",
       "Threshold:  0.5\n",
       "File Saved:  stop 150 0.5\n",
       "Threshold:  0.9\n",
       "File Saved:  stop 150 0.9\n",
       "Threshold:  0.999\n",
       "File Saved:  stop 150 0.999\n",
       "Features:  200\n",
       "Threshold:  0.5\n",
       "File Saved:  stop 200 0.5\n",
       "Threshold:  0.9\n",
       "File Saved:  stop 200 0.9\n",
       "Threshold:  0.999\n",
       "File Saved:  stop 200 0.999\n",
       "Features:  250\n",
       "Threshold:  0.5\n",
       "File Saved:  stop 250 0.5\n",
       "Threshold:  0.9\n",
       "File Saved:  stop 250 0.9\n",
       "Threshold:  0.999\n",
       "File Saved:  stop 250 0.999\n",
       "Features:  300\n",
       "Threshold:  0.5\n",
       "File Saved:  stop 300 0.5\n",
       "Threshold:  0.9\n",
       "File Saved:  stop 300 0.9\n",
       "Threshold:  0.999\n",
       "File Saved:  stop 300 0.999\n",
       "Features:  350\n",
       "Threshold:  0.5\n",
       "File Saved:  stop 350 0.5\n",
       "Threshold:  0.9\n",
       "File Saved:  stop 350 0.9\n",
       "Threshold:  0.999\n",
       "File Saved:  stop 350 0.999\n",
       "Features:  400\n",
       "Threshold:  0.5\n",
       "File Saved:  stop 400 0.5\n",
       "Threshold:  0.9\n",
       "File Saved:  stop 400 0.9\n",
       "Threshold:  0.999\n",
       "File Saved:  stop 400 0.999\n",
       "Features:  450\n",
       "Threshold:  0.5\n",
       "File Saved:  stop 450 0.5\n",
       "Threshold:  0.9\n",
       "File Saved:  stop 450 0.9\n",
       "Threshold:  0.999\n",
       "File Saved:  stop 450 0.999\n",
       "Features:  500\n",
       "Threshold:  0.5\n",
       "File Saved:  stop 500 0.5\n",
       "Threshold:  0.9\n",
       "File Saved:  stop 500 0.9\n",
       "Threshold:  0.999\n",
       "File Saved:  stop 500 0.999\n",
       "Features:  550\n",
       "Threshold:  0.5\n",
       "File Saved:  stop 550 0.5\n",
       "Threshold:  0.9\n",
       "File Saved:  stop 550 0.9\n",
       "Threshold:  0.999\n",
       "File Saved:  stop 550 0.999\n",
       "Features:  600\n",
       "Threshold:  0.5\n",
       "File Saved:  stop 600 0.5\n",
       "Threshold:  0.9\n",
       "File Saved:  stop 600 0.9\n",
       "Threshold:  0.999\n",
       "File Saved:  stop 600 0.999\n",
       "Features:  650\n",
       "Threshold:  0.5\n",
       "File Saved:  stop 650 0.5\n",
       "Threshold:  0.9\n",
       "File Saved:  stop 650 0.9\n",
       "Threshold:  0.999\n",
       "File Saved:  stop 650 0.999\n",
       "Features:  700\n",
       "Threshold:  0.5\n",
       "File Saved:  stop 700 0.5\n",
       "Threshold:  0.9\n",
       "File Saved:  stop 700 0.9\n",
       "Threshold:  0.999\n",
       "File Saved:  stop 700 0.999\n"
      ]
     }
   ],
   "source": [
    "classify(lemm_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
