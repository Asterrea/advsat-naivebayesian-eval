{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check feature extraction tutorial first before this. \n",
    "\n",
    "After feature extraction, it will generate a csv file that has:\n",
    "1. Term\n",
    "2. Number of legitimate emails that word occurred in\n",
    "3. Number of spam emails that word occurred in\n",
    "4. Mutual Information Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing the mutual information score of each term, we can now extract the top n highest terms based on the MI to be used as the features for the classifier. The paper used 50-700 features, step 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, check MI folder\n"
     ]
    }
   ],
   "source": [
    "numMI = [50,100,150,200,250,300,350,400,450,500,550,600,650,700]\n",
    "corpus = ['bare','lemm', 'lemm_stop', 'stop']\n",
    "\n",
    "for corp in corpus:\n",
    "    for num in numMI:\n",
    "        termMIList = pd.read_csv(\"Features/\"+corp+\"/\"+corp+\"termMI.csv\", index_col = 0)\n",
    "        terms = pd.DataFrame(termMIList.head(n=num)).to_csv(\"MI/\"+corp+\"/\"+str(num)+\"terms.csv\")\n",
    "print (\"Done, check MI folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to classify, insert formula hereeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from decimal import *\n",
    "\n",
    "getcontext().prec = 256\n",
    "\n",
    "\n",
    "# A = P(X=x|C=c)\n",
    "# P(A|B), is equal to P(AB)/P(B).\n",
    "# P(A) = (Total number of times x occurred/total number of term occurrence in the corpus)\n",
    "# P(B) = (Total number of email c in the corpus /total number of documents in the corpus)\n",
    "#termProb = total number of times x occurred  in document\n",
    "\n",
    "\n",
    "def computeTermGivenClass(probClass, terms, totalTerms):\n",
    "    prob = Decimal(1.0)\n",
    "    for x in terms:\n",
    "        a = (Decimal(x)/Decimal(totalTerms))\n",
    "        prob = (Decimal(prob) * (Decimal(a)/Decimal(probClass)))\n",
    "    \n",
    "    return prob\n",
    "\n",
    "#B = P(X=x) = (Total number of times x occurred in the corpus)/(Total number of word occurrence in the corpus)\n",
    "#C = P(C=c) = (Total number of documents that are c in the corpus)/(Total number of documents in the corpus)\n",
    "          \n",
    "def computeProbability(isComputingSpam, totalDoc, spamTerms, legitTerms, totalTerms, totalLegitCount, totalSpamCount):\n",
    "      \n",
    "    probSpam = (totalSpamCount/totalDoc)\n",
    "    probLegit = (totalLegitCount/totalDoc)\n",
    "    \n",
    "    givenSpam =  (Decimal(probSpam) * computeTermGivenClass(probSpam, spamTerms, totalTerms))\n",
    "    givenLegit = (Decimal(probLegit) * computeTermGivenClass(probLegit, legitTerms, totalTerms))\n",
    "    \n",
    "    if isComputingSpam == True:\n",
    "        numerator =  givenSpam\n",
    "    else:\n",
    "        numerator =  givenLegit\n",
    "            \n",
    "    denominator = Decimal(givenSpam) + Decimal(givenLegit)\n",
    "    \n",
    "    probClass = Decimal(numerator/denominator)\n",
    "    return probClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def extractWords(filepath):\n",
    "    file = open(filepath, 'r')\n",
    "    # .lower() returns a version with all upper case characters replaced with lower case characters.\n",
    "    text = file.read().lower()\n",
    "    file.close()\n",
    "    # replaces anything that is not a lowercase letter, a space, or an apostrophe with a space:\n",
    "    text = re.sub('[^a-z]+', \" \", text)\n",
    "    words = list(text.split())\n",
    "    \n",
    "     # remove duplicate words in the list\n",
    "    words = list(set(words))\n",
    "    # removes words that are less than 4 letters/characters\n",
    "    words =  [i for i in words if len(i) >= 4] \n",
    "    return words;\n",
    "\n",
    "def findTermCounts(terms, docTerms):\n",
    "    return terms.loc[terms['Term'].isin(docTerms)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "numFeatures = [50,100,150,200,250,300,350,400,450,500,550,600,650,700]\n",
    "# 0.5 - 1, 0.9 - 9, 0.999 - 999\n",
    "threshold = [0.5, 0.9, 0.999]\n",
    "\n",
    "pattern = re.compile(\"\\d+-\\d+msg\\d+.txt\")\n",
    "\n",
    "def classify(corp):\n",
    "    #load the vocabulary/word/term list for the entire corpus from file\n",
    "    corpusTerms = pd.read_csv(\"Features/\"+corp.corpusName+\"/baretermMI.csv\", index_col = 0)\n",
    "    totalLegitTerms = corpusTerms['LegitCount'].sum(axis=0)\n",
    "    totalSpamTerms = corpusTerms['SpamCount'].sum(axis=0)\n",
    "\n",
    "    print (\"Classifying Documents in Corpus: \", corp.corpusName)\n",
    "    \n",
    "    totalDoc = corp.totalEmailCtr\n",
    "    totalLegitCount = corp.legitEmailCtr\n",
    "    totalSpamCount = corp.spamEmailCtr\n",
    "    \n",
    "    for num in numFeatures:\n",
    "        terms = pd.read_csv(\"MI/\"+corp.corpusName+\"/\"+str(num)+\"terms.csv\", index_col = 0)\n",
    "        print (\"Using\", len(terms), \" of Features\") \n",
    "        spamCount = terms['SpamCount'].tolist()\n",
    "        legitCount = terms['LegitCount'].tolist()\n",
    "        totalTerms = totalLegitCount + totalSpamCount\n",
    "        \n",
    "        for t in threshold:\n",
    "            actual = []\n",
    "            predicted = []\n",
    "            #for each subdirectory in a corpus (folders - part 1 - 10)\n",
    "            for subdir, dirs, files in os.walk(rootdir):\n",
    "            #for each file in a folder\n",
    "                for file in files:  \n",
    "                    filepath = subdir +\"/\"+file       \n",
    "                    \n",
    "                    docTerms = extractWords(filepath)\n",
    "                    rowTerms = findTermCounts(terms, docTerms)\n",
    "                    PCSpam = Decimal(computeProbability(True, totalDoc, spamCount, legitCount, \n",
    "                                                        totalTerms, totalLegitCount, totalSpamCount))\n",
    "                    PCLegit = Decimal(computeProbability(False, totalDoc, spamCount, legitCount, \n",
    "                                                        totalTerms, totalLegitCount, totalSpamCount))\n",
    "                    \n",
    "                    if PCSpam/PCLegit > t:\n",
    "                        p = 0\n",
    "                    else:\n",
    "                        p = 1\n",
    "                        \n",
    "                    if pattern.match(file): \n",
    "                        a = 1\n",
    "                    else:\n",
    "                        a = 0\n",
    "                    \n",
    "                    actual.append(a)\n",
    "                    predicted.append(p)\n",
    "                    \n",
    "        results = pd.DataFrame(\n",
    "            {'Actual': actual,\n",
    "             'Predicted' : predicted,\n",
    "            })\n",
    "\n",
    "        #save the Term MI to CSV (so we can access it later)\n",
    "        results.to_csv(\"Classified/\"+corp.corpusName+\"/\"+str(num)+\"/\"+str(t)+\"_results.csv\")\n",
    "        print(\"File Saved: \", corp, num, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying Documents in Corpus:  bare\n",
      "Using 50  of Features\n",
      "File Saved:  <__main__.CorpusData object at 0x7ff87d4246a0> 50 0.999\n",
      "Using 100  of Features\n",
      "File Saved:  <__main__.CorpusData object at 0x7ff87d4246a0> 100 0.999\n",
      "Using 150  of Features\n",
      "File Saved:  <__main__.CorpusData object at 0x7ff87d4246a0> 150 0.999\n",
      "Using 200  of Features\n",
      "File Saved:  <__main__.CorpusData object at 0x7ff87d4246a0> 200 0.999\n",
      "Using 250  of Features\n",
      "File Saved:  <__main__.CorpusData object at 0x7ff87d4246a0> 250 0.999\n",
      "Using 300  of Features\n",
      "File Saved:  <__main__.CorpusData object at 0x7ff87d4246a0> 300 0.999\n",
      "Using 350  of Features\n"
     ]
    }
   ],
   "source": [
    "# create a class for the the Corpus data, it will store the total number of emails in the corpus, \n",
    "# along with the total number of spam and legit emails\n",
    "class CorpusData: \n",
    "    corpusName = \"\"\n",
    "    totalEmailCtr = 0\n",
    "    spamEmailCtr = 0\n",
    "    legitEmailCtr = 0\n",
    "\n",
    "    def __init__(self, corpusName, totalEmailCtr, spamEmailCtr, legitEmailCtr):\n",
    "        self.corpusName = corpusName\n",
    "        self.totalEmailCtr = totalEmailCtr\n",
    "        self.spamEmailCtr = spamEmailCtr\n",
    "        self.legitEmailCtr = legitEmailCtr\n",
    "        \n",
    "# total emails, total spam emails, total legit emails\n",
    "bare = CorpusData(\"bare\", 2515, 304, 2211)\n",
    "lemm = CorpusData(\"lemm\", 2776, 452, 2324)\n",
    "lemm_stop = CorpusData(\"lemm_stop\", 2609, 281, 2409)\n",
    "stop = CorpusData(\"stop\", 2341, 481, 1860)\n",
    "\n",
    "corpusDataList = []\n",
    "\n",
    "corpusDataList.append(bare)\n",
    "corpusDataList.append(lemm)\n",
    "corpusDataList.append(lemm_stop)\n",
    "corpusDataList.append(stop)\n",
    "\n",
    "for corp in corpusDataList:\n",
    "    classify(corp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "0.0012629999999980157\n",
      "0.0 0.0 0.0012629999999980157\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# rootdir = \"Emails/\"\n",
    "\n",
    "# #for each subdirectory in a corpus (folders - part 1 - 10)\n",
    "# for subdir, dirs, files in os.walk(rootdir):\n",
    "# #for each file in a folder\n",
    "#     for file in files:  \n",
    "#         filepath = subdir +\"/\"+file    \n",
    "#         print (filepath)\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.clock()\n",
    "print(\"hello\")\n",
    "end = time.clock()\n",
    "print(end - start)\n",
    "\n",
    "m, s = divmod(end - start, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print (h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
