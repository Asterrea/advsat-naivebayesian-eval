{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Tutorial based from the paper : \"An Evaluation of Naive Bayesian Anti-Spam Filtering\n",
    "\n",
    "This is a tutorial to replicate the process of the paper entitled: \"An Evaluation of Naive Bayesian Anti-Spam Filtering. The objective of the paper is to filter spam emails using a Naive Bayes Techinique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training our data to filter out spam, we need to find the features that will be used, the paper used the words found in the corpus as the features of the classifier. We created a whole tutorial on feature extraction alone, refer to another notebook named: Jupyter Feature Extraction. \n",
    "\n",
    "After reading the feature extraction tutorial, you already have the list of all the terms in each corpus with their corresponding Mutual Information score, the following function will extract n-terms with the highest MI. The you/system decide how many features will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, check MI folder\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "numMI = 10\n",
    "corpus = ['bare','lemm', 'lemm_stop', 'stop']\n",
    "\n",
    "for corp in corpus:\n",
    "    termMIList = pd.read_csv(\"Features/\"+corp+\"/\"+corp+\"termMI.csv\", index_col = 0)\n",
    "    terms = pd.DataFrame(termMIList['Term'].head(n=numMI).tolist()).to_csv(\"MI/\"+corp+\"MI.txt\", \n",
    "                                                                    header = None, index = None)\n",
    "print (\"Done, check MI folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our Data\n",
    "\n",
    "The dataset provided to us has 4 directories [bare, lemm, lemm_stop, stop] with subdirectories of 10 parts. Where 9 were used as training set and 1 reserved for testing for every repetition. To reduce random variation, a ten cross-validation was done yielding the ten subdirectories.\n",
    "\n",
    "We could first parse all subdirectories into a list like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# eg. passed directory = 'Emails\\\\bare'\n",
    "# reads its subdirectories and files into a list of lists\n",
    "def parse_subdirectories(directory):\n",
    "    for path, subdirs, files in os.walk(directory):\n",
    "        dir_dataset = []\n",
    "        for filename in files:\n",
    "            f = os.path.join(path,filename)\n",
    "            subdir_content = []\n",
    "            with open(f,'r') as file_content:\n",
    "                content = file_content.read()\n",
    "                subdir_content.append(content)\n",
    "        dir_dataset.append(subdir_content)\n",
    "    return dir_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split this data into 90% training set for Naive Bayes prediction and 10% data for accuracy testing. Let's call these variables: train_set and test_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_dataset(dataset, n_split):\n",
    "    train_set = []\n",
    "    data_copy = list(dataset)\n",
    "    while len(train_set) < int(len(dataset)*n_split):\n",
    "        pointer = random.randrange(len(data_copy))\n",
    "        train_set.append(data_copy.pop(pointer))\n",
    "    return [train_set,data_copy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To form a list of features to finally use in predicting classification using the Naive Bayes theorem, a common feature selection method is done by computing the Mutual Information (MI) of term t and class c \n",
    "\n",
    "where, \n",
    "- t is defined to be a word attribute and;\n",
    "- classified either as c = spam or not spam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
