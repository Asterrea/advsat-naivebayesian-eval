{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Tutorial based from the paper : \"An Evaluation of Naive Bayesian Anti-Spam Filtering\n",
    "\n",
    "This is a tutorial to replicate the process of the paper entitled: \"An Evaluation of Naive Bayesian Anti-Spam Filtering. The objective of the paper is to filter spam emails using a Naive Bayes Techinique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our Data\n",
    "\n",
    "The dataset provided to us has 4 directories [bare, lemm, lemm_stop, stop] with subdirectories of 10 parts. Where 9 were used as training set and 1 reserved for testing for every repetition. To reduce random variation, a ten cross-validation was done yielding the ten subdirectories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To form a list of features to finally use in predicting classification using the Naive Bayes theorem, a common feature selection method is done by computing the Mutual Information (MI) of term t and class c \n",
    "\n",
    "where, \n",
    "- <i>t</i> is defined to be a word attribute and;\n",
    "- classified either as <i>c</i> = spam or not spam. \n",
    "\n",
    "We are given a description <i>d âˆˆ X</i> of a document, where <i>X</i> is the document space; and a fixed set of classes <i>C = {spam, legitimate}</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training our data to filter out spam, we need to find the features that will be used, the paper used the words found in the corpus as the features of the classifier. We created a whole tutorial on feature extraction alone, refer to another notebook named: <b><i>Jupyter Feature Extraction</i></b>. \n",
    "\n",
    "After reading the feature extraction tutorial, you already have the list of all the terms in each corpus with their corresponding Mutual Information score, the following function will extract n-terms with the highest <i>MI</i>. The you/system decide how many features will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, check MI folder\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "numMI = 10\n",
    "corpus = ['bare','lemm', 'lemm_stop', 'stop']\n",
    "\n",
    "for corp in corpus:\n",
    "    termMIList = pd.read_csv(\"Features/\"+corp+\"/\"+corp+\"termMI.csv\", index_col = 0)\n",
    "    terms = pd.DataFrame(termMIList['Term'].head(n=numMI).tolist()).to_csv(\"MI/\"+corp+\"MI.txt\", \n",
    "                                                                    header = None, index = None)\n",
    "print (\"Done, check MI folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the train and test set split by passing the extracted <i>.txt</i> file to the function below that automatically walks through the directory given to it and transfers the contents of the text file into a list called <i>dir_dataset</i>, where the function <i>parse_subdirectories</i> returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# eg. passed directory = 'Emails/bare'\n",
    "# reads its subdirectories and files into a list of lists\n",
    "def parse_subdirectories(directory):\n",
    "    for path, subdirs, files in os.walk(directory):\n",
    "        dir_dataset = []\n",
    "        for filename in files:\n",
    "            f = os.path.join(path,filename)\n",
    "            subdir_content = []\n",
    "            with open(f,'r') as file_content:\n",
    "                content = file_content.read()\n",
    "                subdir_content.append(content)\n",
    "        dir_dataset.append(subdir_content)\n",
    "    return dir_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split this list data into 90% training set and 10% data for accuracy testing of Naive Bayes prediction. Let's call these list variables: <i>train_set</i> and <i>test_set</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_dataset(dataset, n_split):\n",
    "    train_set = []\n",
    "    data_copy = list(dataset)\n",
    "    while len(train_set) < int(len(dataset)*n_split):\n",
    "        pointer = random.randrange(len(data_copy))\n",
    "        train_set.append(data_copy.pop(pointer))\n",
    "    return [train_set,data_copy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Email/Documents\n",
    "\n",
    "To start classifying if the instance of document <i>X</i> is a legitimate message or spam, we first create a <b>Term Matrix</b>. The term matrix contains the final features cross-checked in every document if it exists or not denoted by: <i>0 or 1</i>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "features= []\n",
    "\n",
    "#MIfilepath: 'MI/[filename].txt\n",
    "def readFeatures(MIfilepath):\n",
    "    with open(MIfilepath) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.rstrip()\n",
    "            features.append(line)\n",
    "    f.close()\n",
    "    \n",
    "#corpusdirectory: 'Emails/bare'\n",
    "#corpusname : bare\n",
    "def build_term_matrix(corpusdirectory, corpusname):\n",
    "    directory = os.path.dirname(\"Term Matrix/\")\n",
    "    subdir = os.path.join(directory,corpusname)\n",
    "    if not os.path.exists(subdir):\n",
    "        os.makedirs(subdir)\n",
    "    csvpath = os.path.join(subdir,corpusname)\n",
    "    csvfile = open(csvpath + '.csv', 'w')\n",
    "    csv_writer = csv.writer(csvfile,delimiter=',')\n",
    "    \n",
    "    for dirs,subdirs,files in os.walk(path):\n",
    "        for messages in sorted(files):\n",
    "            f = os.path.join(dirs,messages)\n",
    "            wih open(f,'r') as email:\n",
    "                content = email.read().split(' ')\n",
    "                csv_writer.writerow(messages)\n",
    "                for feature in features:\n",
    "                    r = csv.reader(open(csvpath+'.csv'))\n",
    "                    if feature in content:\n",
    "                        if os.path.exists(csvpath+'.csv'):\n",
    "                            for row in r:\n",
    "                                row.append('1')\n",
    "                    else:\n",
    "                        if os.path.exists(csvpath+'.csv'):\n",
    "                            for row in r:\n",
    "                                row.append('0')\n",
    "            email.close()\n",
    "        csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
